{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# For data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For splitting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For resampling (handling class imbalance)\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "\n",
    "# For scaling numeric features\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from lightgbm.callback import early_stopping  # Correct import for the early stopping callback\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# MLFlow for MLOps\n",
    "import mlflow\n",
    "import mlflow.sklearn  # For scikit-learn models\n",
    "import mlflow.catboost  # For CatBoost models\n",
    "import mlflow.lightgbm  # For LightGBM models\n",
    "import mlflow.xgboost  # For XGBoost models\n",
    "\n",
    "# Configurations (optional)\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/Train.csv')\n",
    "test = pd.read_csv('data/Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Loan_Status'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows\n",
    "print(train.head())\n",
    "\n",
    "# Get a concise summary of the DataFrame\n",
    "print(train.info())\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = train.isnull().sum()\n",
    "print(\"Missing Values:\\n\", missing_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "1. No Missing Values: None of the columns have missing values, so we donâ€™t need imputation for this dataset.\n",
    "2. Data Types:\n",
    "    * Most columns are numeric (int64, float64).\n",
    "    * Loan_ID and Dependents are object types and may need preprocessing (e.g., encoding or converting to numerical values where necessary).\n",
    "3. Features:\n",
    "    * ID and Loan_ID appear to be identifiers and might not contribute to the predictive power of the model.\n",
    "    * Categorical features such as Gender, Married, Dependents, Education, Self_Employed, and Property_Area might need encoding or exploration to understand their distributions.\n",
    "    * Numerical features (ApplicantIncome, CoapplicantIncome, LoanAmount, etc.) can be further analyzed for distributions and relationships with Loan_Status.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate, Bivariate, and Multivariate Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exploring the Loan_ID column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Examine unique values and patterns\n",
    "print(\"Sample Loan_IDs:\")\n",
    "print(train['Loan_ID'].head(10))\n",
    "\n",
    "# Step 2: Extract prefix and suffix (assuming alphanumeric IDs)\n",
    "train['Loan_ID_Prefix'] = train['Loan_ID'].str.extract(r'([A-Za-z]+)')  # Extract alphabetic prefix\n",
    "train['Loan_ID_Suffix'] = train['Loan_ID'].str.extract(r'(\\d+)')       # Extract numeric suffix\n",
    "\n",
    "# Step 3: Analyze the extracted components\n",
    "print(\"\\nUnique prefixes:\")\n",
    "print(train['Loan_ID_Prefix'].unique())\n",
    "\n",
    "print(\"\\nSummary of suffixes:\")\n",
    "print(train['Loan_ID_Suffix'].astype(int).describe())\n",
    "\n",
    "# Step 4: Visualize the prefix distribution\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=train, x='Loan_ID_Prefix', order=train['Loan_ID_Prefix'].value_counts().index)\n",
    "plt.title('Distribution of Loan_ID Prefixes')\n",
    "plt.xlabel('Loan_ID Prefix')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Step 5: Check the correlation with Loan_Status\n",
    "prefix_status = train.groupby(['Loan_ID_Prefix', 'Loan_Status']).size().unstack(fill_value=0)\n",
    "print(\"\\nLoan Status Distribution by Prefix:\")\n",
    "print(prefix_status)\n",
    "\n",
    "# Visualize the relationship between prefixes and Loan_Status\n",
    "prefix_status_normalized = prefix_status.div(prefix_status.sum(axis=1), axis=0)\n",
    "prefix_status_normalized.plot(kind='bar', stacked=True, figsize=(12, 6), colormap='viridis')\n",
    "plt.title('Loan Status Proportions by Loan_ID Prefix')\n",
    "plt.ylabel('Proportion')\n",
    "plt.xlabel('Loan_ID Prefix')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract numeric suffix\n",
    "train['Loan_ID_Suffix'] = train['Loan_ID'].str.extract(r'(\\d+)$').astype(int)\n",
    "test['Loan_ID_Suffix'] = test['Loan_ID'].str.extract(r'(\\d+)$').astype(int)\n",
    "\n",
    "# Step 2: Analyze the suffix\n",
    "print(\"Train Loan_ID_Suffix Statistics:\")\n",
    "print(train['Loan_ID_Suffix'].describe())\n",
    "\n",
    "print(\"\\nTest Loan_ID_Suffix Statistics:\")\n",
    "print(test['Loan_ID_Suffix'].describe())\n",
    "\n",
    "# Visualize distribution of suffix\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(train['Loan_ID_Suffix'], bins=20, kde=True, color=\"purple\")\n",
    "plt.title(\"Distribution of Loan_ID Suffix in Train Dataset\")\n",
    "plt.xlabel(\"Loan_ID Suffix\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Step 3: Correlation with Loan_Status\n",
    "suffix_corr = train[['Loan_ID_Suffix', 'Loan_Status']].corr()\n",
    "print(\"\\nCorrelation between Loan_ID_Suffix and Loan_Status:\")\n",
    "print(suffix_corr)\n",
    "\n",
    "# Step 4: Create bins for Loan_ID_Suffix\n",
    "train['Loan_ID_Suffix_Bin'] = pd.qcut(train['Loan_ID_Suffix'], q=4, labels=['Early', 'Mid-Early', 'Mid-Late', 'Late'])\n",
    "test['Loan_ID_Suffix_Bin'] = pd.qcut(test['Loan_ID_Suffix'], q=4, labels=['Early', 'Mid-Early', 'Mid-Late', 'Late'])\n",
    "\n",
    "# Visualize Loan_Status proportions by Loan_ID_Suffix_Bin\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=train, x='Loan_ID_Suffix_Bin', hue='Loan_Status', palette='viridis')\n",
    "plt.title(\"Loan_Status Proportions by Loan_ID Suffix Bins\")\n",
    "plt.xlabel(\"Loan_ID Suffix Bins\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Step 5: Feature encoding for Loan_ID_Suffix_Bin\n",
    "train = pd.get_dummies(train, columns=['Loan_ID_Suffix_Bin'], drop_first=True)\n",
    "test = pd.get_dummies(test, columns=['Loan_ID_Suffix_Bin'], drop_first=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations\n",
    "\n",
    "1. Distribution:\n",
    "    * The Loan_ID_Suffix values are not evenly distributed; they exhibit a clustering pattern.\n",
    "    * Most suffixes are concentrated around higher values, as evident from the histogram.\n",
    "\n",
    "2. Binned Suffix Categories:\n",
    "    * Suffixes were grouped into four bins: Early, Mid-Early, Mid-Late, and Late.\n",
    "    * These bins correspond to ranges within the Loan_ID_Suffix and allow us to analyze loan approval rates across different segments.\n",
    "\n",
    "3. Loan Approval by Suffix Bins:\n",
    "    * Loan_Status proportions in all suffix bins are consistent, with a higher number of approved loans (Loan_Status = 1) compared to rejected loans (Loan_Status = 0).\n",
    "    * However, no significant variation is observed between bins, suggesting that Loan_ID_Suffix may not strongly influence loan approval directly.\n",
    "\n",
    "4. Correlation with Loan_Status:\n",
    "    * The correlation coefficient between Loan_ID_Suffix and Loan_Status is -0.029, indicating a negligible linear relationship.\n",
    "\n",
    "5. Train vs Test Statistics:\n",
    "* The summary statistics (mean, median, range) for Loan_ID_Suffix in both train and test datasets are very similar, suggesting consistent data distribution across both datasets.\n",
    "\n",
    "Next Steps:\n",
    "* Feature Engineering: While the Loan_ID_Suffix feature doesn't directly correlate with Loan_Status, its bins can be used as categorical features in the modeling process to capture any non-linear trends.\n",
    "* Validation: Assess the importance of this feature during model training to decide whether to keep or drop it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure bins are consistent for train and test datasets\n",
    "bins = [1000, 1500, 2000, 2500, 2602]\n",
    "labels = ['Early', 'Mid-Early', 'Mid-Late', 'Late']\n",
    "\n",
    "# Apply binning\n",
    "train['Loan_ID_Bin'] = pd.cut(train['Loan_ID_Suffix'], bins=bins, labels=labels, include_lowest=True)\n",
    "test['Loan_ID_Bin'] = pd.cut(test['Loan_ID_Suffix'], bins=bins, labels=labels, include_lowest=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one-hot encoding\n",
    "train = pd.get_dummies(train, columns=['Loan_ID_Bin'], prefix='LoanID')\n",
    "test = pd.get_dummies(test, columns=['Loan_ID_Bin'], prefix='LoanID')\n",
    "\n",
    "# Ensure train and test have the same columns (in case one bin is missing in the test set)\n",
    "train, test = train.align(test, join='outer', axis=1, fill_value=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first few rows of the train and test datasets\n",
    "print(train.head())\n",
    "print(test.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exploring the Gender column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Gender\n",
    "print(train['Gender'].value_counts())\n",
    "\n",
    "# Gender vs Loan_Status\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.countplot(x='Gender', hue='Loan_Status', data=train, palette='viridis')\n",
    "plt.title('Gender Distribution by Loan Status')\n",
    "plt.xlabel('Gender (0=Female, 1=Male)')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Loan Status', labels=['Not Approved', 'Approved'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "1. Gender Distribution:\n",
    "    * The dataset is heavily skewed towards males (1), with a count of 5372 compared to 526 females (0).\n",
    "    * This imbalance might mean that the model could learn more about the loan approval patterns of males than females unless balanced in some way.\n",
    "\n",
    "2. Loan Status by Gender:\n",
    "    * Both males and females have higher counts in the approved (1) class than in the not approved (0) class.\n",
    "    * However, the majority of loan applicants and approvals are male, which might reflect trends in the dataset or underlying societal patterns.\n",
    "\n",
    "\n",
    "Discussion:\n",
    "While Gender is somewhat imbalanced, it might still provide useful information for the model, particularly in conjunction with other features (e.g., Married, ApplicantIncome, etc.). The following steps are possible:\n",
    "\n",
    "1. Retain Gender as-is:\n",
    "    * If no significant preprocessing is required, we can keep the feature as-is for now. It is already encoded as 0 and 1.\n",
    "\n",
    "2. Transformations:\n",
    "    * If we suspect imbalance might affect model predictions, consider balancing methods later (e.g., SMOTE for oversampling the minority class).\n",
    "3. Feature Interaction:\n",
    "    * Investigate how Gender interacts with other features (e.g., ApplicantIncome, Married, etc.) to uncover deeper insights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender and Married Interaction\n",
    "sns.countplot(x='Married', hue='Gender', data=train, palette='viridis')\n",
    "plt.title('Gender vs Married')\n",
    "plt.xlabel('Married (0=No, 1=Yes)')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Gender', labels=['Female', 'Male'])\n",
    "plt.show()\n",
    "\n",
    "# Proportion of Married Applicants by Gender\n",
    "gender_married = train.groupby(['Gender', 'Married'])['Loan_Status'].value_counts(normalize=True).unstack()\n",
    "print(\"Proportion of Loan Status by Gender and Married:\\n\", gender_married)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applicant Income by Gender\n",
    "sns.boxplot(x='Gender', y='ApplicantIncome', data=train, palette='viridis')\n",
    "plt.title('Applicant Income Distribution by Gender')\n",
    "plt.xlabel('Gender (0=Female, 1=Male)')\n",
    "plt.ylabel('Applicant Income')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender vs Credit_History\n",
    "sns.countplot(x='Credit_History', hue='Gender', data=train, palette='viridis')\n",
    "plt.title('Gender vs Credit History')\n",
    "plt.xlabel('Credit History (0=No, 1=Yes)')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Gender', labels=['Female', 'Male'])\n",
    "plt.show()\n",
    "\n",
    "# Proportion of Loan Status by Gender and Credit History\n",
    "gender_credit = train.groupby(['Gender', 'Credit_History'])['Loan_Status'].value_counts(normalize=True).unstack()\n",
    "print(\"Proportion of Loan Status by Gender and Credit History:\\n\", gender_credit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gender_Married interaction feature\n",
    "train['Gender_Married'] = train['Gender'].astype(str) + \"_\" + train['Married'].astype(str)\n",
    "\n",
    "# Create Gender_Credit_History interaction feature\n",
    "train['Gender_Credit_History'] = train['Gender'].astype(str) + \"_\" + train['Credit_History'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare data for feature importance evaluation\n",
    "X = train.drop(['Loan_Status', 'ID', 'Loan_ID'], axis=1)\n",
    "y = train['Loan_Status']\n",
    "\n",
    "# Encode categorical features\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "# Plot feature importances\n",
    "importances[:10].plot(kind='bar', figsize=(10, 6), title=\"Top 10 Feature Importances\")\n",
    "plt.ylabel(\"Importance Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of Gender Interactions\n",
    "1. Gender vs Married:\n",
    "The majority of applicants are males who are married.\n",
    "Loan approval rates are similar across categories, with no significant difference between married and unmarried individuals, regardless of gender (around 83%-84% approval rate for all groups).\n",
    "2. Gender vs ApplicantIncome:\n",
    "Males have higher median ApplicantIncome compared to females.\n",
    "There are several outliers for both genders, with males having some exceptionally high incomes (>80,000).\n",
    "Income might be more predictive of loan status when combined with gender, as males appear to earn more overall.\n",
    "3. Gender vs Credit History:\n",
    "Most applicants (both male and female) have a credit history (Credit_History = 1), and the approval rate is highest among those with a credit history (>83%).\n",
    "Gender does not seem to significantly impact the approval rate within the same credit history category.\n",
    "4. Feature Importance (Random Forest):\n",
    "ApplicantIncome and Loan_Amount_Term are the most important features.\n",
    "The new interaction features (Gender_Married and Gender_Credit_History) are not in the top 10, indicating that Gender alone or its interactions might have limited predictive power.\n",
    "\n",
    "Recommendations for Gender Feature\n",
    "1. Retain Gender as-is:\n",
    "It might still contribute to the model in combination with other features like income or credit history.\n",
    "2. Drop Interaction Features:\n",
    "The interaction features (Gender_Married and Gender_Credit_History) did not show strong predictive power based on feature importance. We can remove them for simplicity unless further exploration proves otherwise.\n",
    "3. Transform Applicant Income: To reduce the effect of outliers, we can apply a log transformation to ApplicantIncome before using it in the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation Steps: \n",
    "Letâ€™s apply the transformations and clean up interaction features:\n",
    "\n",
    "> Step 1: Remove Interaction Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Gender_Married and Gender_Credit_History if created\n",
    "train.drop(['Gender_Married', 'Gender_Credit_History'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Step 2: Log Transform Applicant Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transformation to ApplicantIncome\n",
    "train['Log_ApplicantIncome'] = np.log1p(train['ApplicantIncome'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Step 3: Validate Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the changes\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exploring Married Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Step 1: Distribution of Married: \n",
    "Weâ€™ll begin by analyzing the distribution of Married and its relationship with Loan_Status. Run the following code:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Married\n",
    "print(train['Married'].value_counts())\n",
    "\n",
    "# Married vs Loan_Status\n",
    "sns.countplot(x='Married', hue='Loan_Status', data=train, palette='viridis')\n",
    "plt.title('Married Distribution by Loan Status')\n",
    "plt.xlabel('Married (0=No, 1=Yes)')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Loan Status', labels=['Not Approved', 'Approved'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Step 2: Interaction with Other Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction: Married and Gender\n",
    "sns.countplot(x='Married', hue='Gender', data=train, palette='viridis')\n",
    "plt.title('Married vs Gender')\n",
    "plt.xlabel('Married (0=No, 1=Yes)')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Gender', labels=['Female', 'Male'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applicant Income by Married\n",
    "sns.boxplot(x='Married', y='ApplicantIncome', data=train, palette='viridis')\n",
    "plt.title('Applicant Income Distribution by Married Status')\n",
    "plt.xlabel('Married (0=No, 1=Yes)')\n",
    "plt.ylabel('Applicant Income')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Married vs Credit_History\n",
    "sns.countplot(x='Credit_History', hue='Married', data=train, palette='viridis')\n",
    "plt.title('Married vs Credit History')\n",
    "plt.xlabel('Credit History (0=No, 1=Yes)')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Married', labels=['No', 'Yes'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportion of Loan_Status by Married\n",
    "married_loan = train.groupby('Married')['Loan_Status'].value_counts(normalize=True).unstack()\n",
    "print(\"Proportion of Loan Status by Married:\\n\", married_loan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of Married Feature\n",
    "1. Marital Status Distribution:\n",
    "    * The majority of applicants are married (5040 vs. 858 not married).\n",
    "    * Married applicants have a slightly lower loan approval rate (83.17%) compared to unmarried applicants (84.03% However, this difference is negligible.\n",
    "2. Married vs Gender:\n",
    "    * Most married applicants are male, reinforcing the observation from the Gender analysis.\n",
    "    * Since both Gender and Married have a strong imbalance, any interaction between them might not add significant value.\n",
    "3. Married vs Applicant Income:\n",
    "    * Married applicants tend to have higher median incomes than unmarried applicants.\n",
    "    * Outliers exist for both groups, but these outliers are more prominent for married individuals.\n",
    "4. Married vs Credit History:\n",
    "    * The majority of married applicants have a credit history (Credit_History=1), and the approval rates are consistently higher for applicants with a credit history.\n",
    "    * Being married doesnâ€™t appear to strongly influence the loan outcome when compared to credit history.\n",
    "5. Loan Approval Proportions:\n",
    "    * The loan approval rates are similar across married and unmarried groups, with no significant variation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommendations for Married\n",
    "1. Retain Married:\n",
    "    * Despite the limited influence, the feature may still contribute when combined with other variables, so we retain it as-is for now.\n",
    "2. No Interaction Features:\n",
    "    * Interactions such as Married_Gender or Married_Credit_History donâ€™t seem to add significant value based on the current observations and approval proportions. We can avoid creating these features unless further modeling experiments suggest otherwise.\n",
    "3. No Transformations Needed:\n",
    "    * Married is already encoded (0=No, 1=Yes), and no additional transformations are required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exploring the Dependents feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Dependents\n",
    "print(train['Dependents'].value_counts())\n",
    "\n",
    "# Dependents vs Loan_Status\n",
    "sns.countplot(x='Dependents', hue='Loan_Status', data=train, palette='viridis')\n",
    "plt.title('Dependents Distribution by Loan Status')\n",
    "plt.xlabel('Dependents')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Loan Status', labels=['Not Approved', 'Approved'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applicant Income by Dependents\n",
    "sns.boxplot(x='Dependents', y='ApplicantIncome', data=train, palette='viridis')\n",
    "plt.title('Applicant Income Distribution by Dependents')\n",
    "plt.xlabel('Dependents')\n",
    "plt.ylabel('Applicant Income')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependents vs Married\n",
    "sns.countplot(x='Dependents', hue='Married', data=train, palette='viridis')\n",
    "plt.title('Dependents Distribution by Marital Status')\n",
    "plt.xlabel('Dependents')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Married', labels=['No', 'Yes'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependents vs Credit_History\n",
    "sns.countplot(x='Dependents', hue='Credit_History', data=train, palette='viridis')\n",
    "plt.title('Dependents vs Credit History')\n",
    "plt.xlabel('Dependents')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Credit History', labels=['No', 'Yes'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportion of Loan_Status by Dependents\n",
    "dependents_loan = train.groupby('Dependents')['Loan_Status'].value_counts(normalize=True).unstack()\n",
    "print(\"Proportion of Loan Status by Dependents:\\n\", dependents_loan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of Dependents Feature\n",
    "1. Dependents Distribution:\n",
    "    * The majority of applicants have no dependents (0 = 3659), followed by 1 dependent (1157).\n",
    "    * Categories 2 and 3+ are smaller, but still relevant.\n",
    "2. Dependents and Loan Status:\n",
    "    * Loan approval rates decrease slightly as the number of dependents increases:\n",
    "        * 0 dependents: ~84% approval.\n",
    "        * 3+ dependents: ~81.7% approval.\n",
    "    * This suggests a possible inverse relationship between the number of dependents and loan approval probability.\n",
    "3. Dependents vs Applicant Income:\n",
    "    * Applicants with more dependents tend to have slightly higher median incomes. This makes sense as families with more dependents may have more earning members or require higher incomes to support their households.\n",
    "4. Dependents vs Marital Status:\n",
    "    * Married applicants are more likely to report dependents, especially in the 1, 2, and 3+ categories.\n",
    "    * Single applicants overwhelmingly fall into the 0 dependents category.\n",
    "5. Dependents vs Credit History:\n",
    "    * Applicants with a credit history dominate across all dependent categories.\n",
    "    * The proportion of applicants with a good credit history does not vary significantly across categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommendations for Dependents\n",
    "1. Retain Dependents as-is:\n",
    "The feature shows some predictive power, especially with its slight inverse relationship to loan approval.\n",
    "2. Group Rare Categories:\n",
    "Consider grouping 2 and 3+ into a single category (2+) to simplify the feature and address the smaller size of these groups.\n",
    "3. No Interaction Features:\n",
    "While Dependents interacts with Married and Credit_History, these interactions do not appear to add significant predictive power. For now, avoid creating interaction features.\n",
    "4. Encode as Integer:\n",
    "If grouping categories, convert Dependents into integers to make it compatible with ML algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Dependents'] = train['Dependents'].replace('3+', '2+')\n",
    "train['Dependents'] = train['Dependents'].replace({'0': 0, '1': 1, '2': 2, '2+': 3}).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exploring the Education feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Education\n",
    "print(train['Education'].value_counts())\n",
    "\n",
    "# Education vs Loan_Status\n",
    "sns.countplot(x='Education', hue='Loan_Status', data=train, palette='viridis')\n",
    "plt.title('Education Distribution by Loan Status')\n",
    "plt.xlabel('Education (0=Undergraduate, 1=Graduate)')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Loan Status', labels=['Not Approved', 'Approved'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applicant Income by Education\n",
    "sns.boxplot(x='Education', y='ApplicantIncome', data=train, palette='viridis')\n",
    "plt.title('Applicant Income Distribution by Education Level')\n",
    "plt.xlabel('Education (0=Undergraduate, 1=Graduate)')\n",
    "plt.ylabel('Applicant Income')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Education vs Dependents\n",
    "sns.countplot(x='Dependents', hue='Education', data=train, palette='viridis')\n",
    "plt.title('Dependents Distribution by Education Level')\n",
    "plt.xlabel('Dependents')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Education', labels=['Undergraduate', 'Graduate'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Education vs Credit_History\n",
    "sns.countplot(x='Credit_History', hue='Education', data=train, palette='viridis')\n",
    "plt.title('Education vs Credit History')\n",
    "plt.xlabel('Credit History (0=No, 1=Yes)')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Education', labels=['Undergraduate', 'Graduate'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportion of Loan_Status by Education\n",
    "education_loan = train.groupby('Education')['Loan_Status'].value_counts(normalize=True).unstack()\n",
    "print(\"Proportion of Loan Status by Education:\\n\", education_loan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of Education Feature\n",
    "1. Education Distribution:\n",
    "    1. The majority of applicants are undergraduates (category 0), significantly outnumbering graduates.\n",
    "    2. This suggests that the dataset is imbalanced in terms of education level.\n",
    "2. Education and Loan Status:\n",
    "    * Graduates have a slightly lower loan approval rate (~82.2%) compared to undergraduates (~83.4%).\n",
    "    * The difference is small but may still hold predictive value.\n",
    "3. Education vs Applicant Income:\n",
    "    * Graduates tend to have slightly higher median incomes than undergraduates, as expected.\n",
    "    * However, the difference is not substantial, and both groups show a similar range of incomes.\n",
    "4. Education vs Dependents:\n",
    "    * A majority of applicants with no dependents are undergraduates, while graduates are distributed more evenly across dependent categories.\n",
    "    * This could indicate that graduates are more likely to have families.\n",
    "5. Education vs Credit History:\n",
    "    * Undergraduates dominate the population with a credit history (Credit_History = 1), reflecting the general imbalance in education levels in the dataset.\n",
    "    * There is no significant variation in credit history proportions between education groups.\n",
    "\n",
    "Recommendations for Education\n",
    "1. Retain Education as-is:\n",
    "The feature is already binary (0 = Undergraduate, 1 = Graduate), which is optimal for most machine learning algorithms.\n",
    "2. No Interaction Features:\n",
    "While Education interacts with other features like Dependents and Credit_History, these interactions do not show significant predictive potential for Loan_Status. No additional interaction features are recommended for now.\n",
    "3. No Additional Transformations:\n",
    "Education is already encoded and well-suited for modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exploring the self_employed column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Self_Employed\n",
    "print(train['Self_Employed'].value_counts())\n",
    "\n",
    "# Self_Employed vs Loan_Status\n",
    "sns.countplot(x='Self_Employed', hue='Loan_Status', data=train, palette='viridis')\n",
    "plt.title('Self_Employed Distribution by Loan Status')\n",
    "plt.xlabel('Self_Employed (0=No, 1=Yes)')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Loan Status', labels=['Not Approved', 'Approved'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applicant Income by Self_Employed\n",
    "sns.boxplot(x='Self_Employed', y='ApplicantIncome', data=train, palette='viridis')\n",
    "plt.title('Applicant Income Distribution by Self-Employment Status')\n",
    "plt.xlabel('Self_Employed (0=No, 1=Yes)')\n",
    "plt.ylabel('Applicant Income')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self_Employed vs Credit_History\n",
    "sns.countplot(x='Credit_History', hue='Self_Employed', data=train, palette='viridis')\n",
    "plt.title('Self_Employed vs Credit History')\n",
    "plt.xlabel('Credit History (0=No, 1=Yes)')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Self_Employed', labels=['No', 'Yes'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self_Employed vs Education\n",
    "sns.countplot(x='Education', hue='Self_Employed', data=train, palette='viridis')\n",
    "plt.title('Self_Employed vs Education')\n",
    "plt.xlabel('Education (0=Undergraduate, 1=Graduate)')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Self_Employed', labels=['No', 'Yes'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportion of Loan_Status by Self_Employed\n",
    "self_employed_loan = train.groupby('Self_Employed')['Loan_Status'].value_counts(normalize=True).unstack()\n",
    "print(\"Proportion of Loan Status by Self_Employed:\\n\", self_employed_loan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of Self_Employed Feature\n",
    "1. Self-Employed Distribution:\n",
    "    * The majority of applicants are not self-employed (0), while a small portion is self-employed (1).\n",
    "    * This imbalance may indicate limited predictive power for Self_Employed on its own.\n",
    "2. Self-Employed and Loan Status:\n",
    "    * Both groups have similar loan approval rates:\n",
    "        * Non-self-employed: ~83.2% approval.\n",
    "        * Self-employed: ~83.9% approval.\n",
    "    * The difference is negligible and may not be a strong differentiator.\n",
    "3. Self-Employed vs Applicant Income:\n",
    "    * Self-employed individuals generally have slightly higher median incomes compared to non-self-employed individuals.\n",
    "    * However, the difference in income distribution between the two groups is not significant.\n",
    "4. Self-Employed vs Credit History:\n",
    "    * Most applicants with a credit history are not self-employed, reflecting the overall distribution imbalance.\n",
    "    * Self-employment status does not seem to strongly correlate with having a credit history.\n",
    "5. Self-Employed vs Education:\n",
    "    * The majority of self-employed individuals are undergraduates, aligning with the dominance of undergraduates in the dataset.\n",
    "\n",
    "\n",
    "Recommendations for Self_Employed\n",
    "1. Retain Self_Employed as-is:\n",
    "Despite the imbalance, the feature may still hold some predictive power when combined with other features like income or credit history.\n",
    "2. No Interaction Features:\n",
    "Interactions such as Self_Employed_Education or Self_Employed_Credit_History donâ€™t show significant value based on current analysis. No additional interaction features are recommended for now.\n",
    "3. No Transformations Required:\n",
    "The feature is already binary (0=No, 1=Yes), and no additional preprocessing is needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exploring the ApplicantIncome feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of ApplicantIncome\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(train['ApplicantIncome'], kde=True, bins=30, color='blue')\n",
    "plt.title('Distribution of Applicant Income')\n",
    "plt.xlabel('Applicant Income')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applicant Income vs Loan Status\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(x='Loan_Status', y='ApplicantIncome', data=train, palette='viridis')\n",
    "plt.title('Applicant Income Distribution by Loan Status')\n",
    "plt.xlabel('Loan Status (0=Not Approved, 1=Approved)')\n",
    "plt.ylabel('Applicant Income')\n",
    "plt.xticks(ticks=[0, 1], labels=['Not Approved', 'Approved'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applicant Income by Self_Employed\n",
    "sns.boxplot(x='Self_Employed', y='ApplicantIncome', data=train, palette='viridis')\n",
    "plt.title('Applicant Income Distribution by Self-Employment Status')\n",
    "plt.xlabel('Self_Employed (0=No, 1=Yes)')\n",
    "plt.ylabel('Applicant Income')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applicant Income by Education\n",
    "sns.boxplot(x='Education', y='ApplicantIncome', data=train, palette='viridis')\n",
    "plt.title('Applicant Income Distribution by Education Level')\n",
    "plt.xlabel('Education (0=Undergraduate, 1=Graduate)')\n",
    "plt.ylabel('Applicant Income')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applicant Income by Dependents\n",
    "sns.boxplot(x='Dependents', y='ApplicantIncome', data=train, palette='viridis')\n",
    "plt.title('Applicant Income Distribution by Number of Dependents')\n",
    "plt.xlabel('Dependents')\n",
    "plt.ylabel('Applicant Income')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transformation of ApplicantIncome\n",
    "train['Log_ApplicantIncome'] = np.log1p(train['ApplicantIncome'])\n",
    "\n",
    "# Visualize transformed distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(train['Log_ApplicantIncome'], kde=True, bins=30, color='green')\n",
    "plt.title('Log-Transformed Distribution of Applicant Income')\n",
    "plt.xlabel('Log Applicant Income')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of ApplicantIncome\n",
    "\n",
    "1. Distribution\n",
    "    * The raw ApplicantIncome is highly skewed with a long tail of high incomes.\n",
    "    * The log transformation significantly normalizes the distribution, making it more suitable for machine learning models.\n",
    "2. Loan Status\n",
    "    * Loan approval is not strongly differentiated by applicant income. Both approved and not-approved groups show a similar income distribution.\n",
    "    * Outliers with very high incomes do not appear to significantly influence loan approval.\n",
    "3. Interactions\n",
    "    * Dependents: Income levels slightly increase with the number of dependents, but the variance within each group is large.\n",
    "    * Education: Graduates tend to have higher incomes compared to undergraduates, as expected.\n",
    "    * Self-Employment: Self-employed individuals generally earn more than non-self-employed ones, aligning with expectations.\n",
    "4. Insights on Transformation\n",
    "    * Using the log-transformed version (Log_ApplicantIncome) is preferred for modeling due to its normalized distribution.\n",
    "    * Creating interaction features (e.g., Income_Education, Income_SelfEmployed) could improve model performance as income patterns vary across these groups.\n",
    "\n",
    "Recommendations\n",
    "1. Retain Log_ApplicantIncome:\n",
    "\n",
    "    *Replace the raw ApplicantIncome with the log-transformed version in the dataset.\n",
    "2. Create Interaction Features:\n",
    "    * Interaction features such as:\n",
    "        * Income_Education = Log_ApplicantIncome * Education\n",
    "        * Income_SelfEmployed = Log_ApplicantIncome * Self_Employed\n",
    "    * These features could add value by capturing combined effects of income and categorical attributes.\n",
    "\n",
    "3. Optional Binning:\n",
    "If interpretability is a priority, binning income into categories (e.g., low, medium, high) can make the feature easier to explain. However, this may lead to loss of information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interaction features\n",
    "train['Income_Education'] = train['Log_ApplicantIncome'] * train['Education']\n",
    "train['Income_SelfEmployed'] = train['Log_ApplicantIncome'] * train['Self_Employed']\n",
    "\n",
    "# Preview the new features\n",
    "print(train[['Log_ApplicantIncome', 'Education', 'Self_Employed', 'Income_Education', 'Income_SelfEmployed']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Income_Education by Loan Status\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(x='Loan_Status', y='Income_Education', data=train, palette='viridis')\n",
    "plt.title('Income_Education Distribution by Loan Status')\n",
    "plt.xlabel('Loan Status (0=Not Approved, 1=Approved)')\n",
    "plt.ylabel('Income_Education')\n",
    "plt.xticks(ticks=[0, 1], labels=['Not Approved', 'Approved'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Income_SelfEmployed by Loan Status\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(x='Loan_Status', y='Income_SelfEmployed', data=train, palette='viridis')\n",
    "plt.title('Income_SelfEmployed Distribution by Loan Status')\n",
    "plt.xlabel('Loan Status (0=Not Approved, 1=Approved)')\n",
    "plt.ylabel('Income_SelfEmployed')\n",
    "plt.xticks(ticks=[0, 1], labels=['Not Approved', 'Approved'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation of interaction features with Loan_Status\n",
    "interaction_features = ['Income_Education', 'Income_SelfEmployed']\n",
    "correlation = train[interaction_features + ['Loan_Status']].corr()\n",
    "print(correlation['Loan_Status'].sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exploring the CoapplicantIncome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot raw distribution of CoapplicantIncome\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(train['CoapplicantIncome'], kde=True, color='blue')\n",
    "plt.title('Distribution of Coapplicant Income')\n",
    "plt.xlabel('Coapplicant Income')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of CoapplicantIncome by Loan_Status\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Loan_Status', y='CoapplicantIncome', data=train, palette='Set2')\n",
    "plt.title('Coapplicant Income Distribution by Loan Status')\n",
    "plt.xlabel('Loan Status (0=Not Approved, 1=Approved)')\n",
    "plt.ylabel('Coapplicant Income')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transformation (add 1 to avoid log(0))\n",
    "train['Log_CoapplicantIncome'] = np.log1p(train['CoapplicantIncome'])\n",
    "\n",
    "# Plot the log-transformed distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(train['Log_CoapplicantIncome'], kde=True, color='green')\n",
    "plt.title('Log-Transformed Distribution of Coapplicant Income')\n",
    "plt.xlabel('Log Coapplicant Income')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot for log-transformed CoapplicantIncome by Loan_Status\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Loan_Status', y='Log_CoapplicantIncome', data=train, palette='Set3')\n",
    "plt.title('Log Coapplicant Income Distribution by Loan Status')\n",
    "plt.xlabel('Loan Status (0=Not Approved, 1=Approved)')\n",
    "plt.ylabel('Log Coapplicant Income')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouped statistics for CoapplicantIncome\n",
    "coapplicant_grouped = train.groupby('Loan_Status')['CoapplicantIncome'].agg(['mean', 'median', 'std', 'min', 'max'])\n",
    "print(coapplicant_grouped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new feature for TotalIncome\n",
    "train['TotalIncome'] = train['ApplicantIncome'] + train['CoapplicantIncome']\n",
    "\n",
    "# Plot distribution of TotalIncome\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(train['TotalIncome'], kde=True, color='purple')\n",
    "plt.title('Distribution of Total Income')\n",
    "plt.xlabel('Total Income')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of TotalIncome by Loan_Status\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Loan_Status', y='TotalIncome', data=train, palette='coolwarm')\n",
    "plt.title('Total Income Distribution by Loan Status')\n",
    "plt.xlabel('Loan Status (0=Not Approved, 1=Approved)')\n",
    "plt.ylabel('Total Income')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation check for TotalIncome and Loan_Status\n",
    "correlation = train[['Loan_Status', 'CoapplicantIncome', 'Log_CoapplicantIncome', 'TotalIncome']].corr()\n",
    "print(correlation['Loan_Status'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations from Visualizations and Statistics:\n",
    "\n",
    "1. Distribution:\n",
    "    * CoapplicantIncome has a right-skewed distribution with most values concentrated at 0, as seen in the histograms.\n",
    "    * The log transformation (Log_CoapplicantIncome) slightly normalizes the distribution but retains a large mass at 0 due to the presence of many zero-income co-applicants.\n",
    "\n",
    "2. Loan Status:\n",
    "    * Boxplots indicate slight differences in CoapplicantIncome and Log_CoapplicantIncome between approved and non-approved loans, though the medians and interquartile ranges suggest limited predictive power.\n",
    "3. Correlation:\n",
    "    * Very low correlations of CoapplicantIncome, Log_CoapplicantIncome, and TotalIncome with Loan_Status suggest weak direct influence.\n",
    "4. Summary Statistics:\n",
    "    * Mean and median differences between loan-approved and non-approved cases for CoapplicantIncome are negligible, supporting the weak predictive potential.\n",
    "\n",
    "Recommendations\n",
    "\n",
    "1. Feature Transformation:\n",
    "    * Consider retaining both Log_CoapplicantIncome and TotalIncome for modeling as they capture different aspects of the income structure, despite weak individual correlation.\n",
    "2. Interaction Features:\n",
    "    * Explore additional interactions with other features, such as the relationship between CoapplicantIncome and marital status, dependents, or education.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exploring the LoanAmount feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Distribution of LoanAmount\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(train['LoanAmount'], kde=True, bins=30, color='blue')\n",
    "plt.title('Distribution of Loan Amount')\n",
    "plt.xlabel('Loan Amount')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# 2. Log Transformation of LoanAmount (to normalize skewness)\n",
    "train['Log_LoanAmount'] = np.log1p(train['LoanAmount'])  # Adding 1 to avoid log(0)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(train['Log_LoanAmount'], kde=True, bins=30, color='green')\n",
    "plt.title('Log-Transformed Distribution of Loan Amount')\n",
    "plt.xlabel('Log Loan Amount')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# 3. LoanAmount Distribution by Loan Status\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=train['Loan_Status'], y=train['LoanAmount'], palette='pastel')\n",
    "plt.title('Loan Amount Distribution by Loan Status')\n",
    "plt.xlabel('Loan Status (0=Not Approved, 1=Approved)')\n",
    "plt.ylabel('Loan Amount')\n",
    "plt.show()\n",
    "\n",
    "# 4. Log LoanAmount Distribution by Loan Status\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x=train['Loan_Status'], y=train['Log_LoanAmount'], palette='Set3')\n",
    "plt.title('Log Loan Amount Distribution by Loan Status')\n",
    "plt.xlabel('Loan Status (0=Not Approved, 1=Approved)')\n",
    "plt.ylabel('Log Loan Amount')\n",
    "plt.show()\n",
    "\n",
    "# 5. Summary statistics for LoanAmount and Log_LoanAmount by Loan_Status\n",
    "summary_stats = train.groupby('Loan_Status')[['LoanAmount', 'Log_LoanAmount']].agg(['mean', 'median', 'std', 'min', 'max'])\n",
    "print(summary_stats)\n",
    "\n",
    "# 6. Correlation with Loan_Status\n",
    "correlations = train[['Loan_Status', 'LoanAmount', 'Log_LoanAmount']].corr()['Loan_Status']\n",
    "print(correlations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "1. Distribution Analysis:\n",
    "\n",
    "    * The LoanAmount feature shows a right-skewed distribution, meaning most values are concentrated towards lower loan amounts, with a long tail towards higher values.\n",
    "    * The log-transformed distribution (Log_LoanAmount) reduces skewness, resulting in a more normalized distribution.\n",
    "2. By Loan Status:\n",
    "\n",
    "    * The mean and median loan amounts are slightly higher for loans that were approved (Loan_Status = 1) compared to those not approved (Loan_Status = 0).\n",
    "    * The standard deviation is high for both groups, indicating variability in loan amounts.\n",
    "    * The log-transformed boxplots further confirm that approved loans tend to have slightly higher loan amounts, but the difference is not stark.\n",
    "3. Summary Statistics:\n",
    "\n",
    "    * LoanAmount has a mean of 98.08 for approved loans and 94.03 for not approved loans.\n",
    "    * Log-transformed values show reduced variability with a mean around 4 for both categories.\n",
    "    * Minimum values (log) and maximum values do not differ significantly across approval categories.\n",
    "4. Correlation with Target:\n",
    "\n",
    "    * The correlation values between Loan_Status and LoanAmount (0.0146) or Log_LoanAmount (0.0102) indicate very weak positive relationships, suggesting minimal direct influence on loan approval.\n",
    "\n",
    "Recommendations for the CoapplicantIncome Feature:\n",
    "\n",
    "1. Retain the Feature:\n",
    "Despite the weak correlation, CoapplicantIncome could still provide useful information in combination with other features, especially interaction terms (e.g., TotalIncome, Income_SelfEmployed, or Income_Education).\n",
    "\n",
    "2. Log Transformation:\n",
    "Use the log-transformed version (Log_CoapplicantIncome) to reduce skewness and normalize the distribution.\n",
    "\n",
    "3. Consider Binning:\n",
    "To capture non-linear relationships, bin Log_CoapplicantIncome into categories (e.g., low, medium, high) and test whether these categories reveal patterns in loan approval.\n",
    "\n",
    "4. Interaction Features:\n",
    "    * Create interaction terms involving CoapplicantIncome, such as:\n",
    "        * TotalIncome = ApplicantIncome + CoapplicantIncome.\n",
    "        * Ratios between CoapplicantIncome and loan amount, or other relevant features.\n",
    "\n",
    "5. Feature Engineering for Zeros:\n",
    "* Since many coapplicants have zero income, create a binary feature like HasCoapplicantIncome to differentiate between those who contribute financially and those who donâ€™t.\n",
    "\n",
    "6. Model Testing:\n",
    "    * Test the feature's contribution during feature importance analysis after training the model. If it remains insignificant, consider dropping it for simplification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exploring the Loan_Amount_Term feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values in Loan_Amount_Term:\", train['Loan_Amount_Term'].isna().sum())\n",
    "\n",
    "# Unique values and value counts\n",
    "print(\"\\nUnique values in Loan_Amount_Term:\")\n",
    "print(train['Loan_Amount_Term'].value_counts())\n",
    "\n",
    "# Plot distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(train['Loan_Amount_Term'].dropna(), kde=False, bins=10, color=\"purple\")\n",
    "plt.title(\"Distribution of Loan_Amount_Term\")\n",
    "plt.xlabel(\"Loan Amount Term (in months)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Boxplot by Loan_Status\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=train, x=\"Loan_Status\", y=\"Loan_Amount_Term\", palette=\"pastel\")\n",
    "plt.title(\"Loan_Amount_Term by Loan_Status\")\n",
    "plt.xlabel(\"Loan Status (0=Not Approved, 1=Approved)\")\n",
    "plt.ylabel(\"Loan Amount Term (in months)\")\n",
    "plt.show()\n",
    "\n",
    "# Grouped summary statistics\n",
    "grouped = train.groupby(\"Loan_Status\")['Loan_Amount_Term'].agg(['mean', 'median', 'std', 'min', 'max'])\n",
    "print(\"\\nSummary statistics for Loan_Amount_Term grouped by Loan_Status:\")\n",
    "print(grouped)\n",
    "\n",
    "# Correlation with Loan_Status\n",
    "correlation = train[['Loan_Status', 'Loan_Amount_Term']].corr()\n",
    "print(\"\\nCorrelation between Loan_Status and Loan_Amount_Term:\")\n",
    "print(correlation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations: \n",
    "\n",
    "1. Distribution Observations:\n",
    "    * The distribution is highly concentrated around a specific value (e.g., 372 or 373 months) with several outliers, suggesting that most loans have similar repayment terms.\n",
    "    * There is a large number of unique values (259), indicating that Loan_Amount_Term is numeric but may have some anomalies or rare values.\n",
    "\n",
    "2. Relationship with Loan_Status:\n",
    "    * Both approved (Loan_Status=1) and not approved (Loan_Status=0) loans have similar mean and median Loan_Amount_Term values (mean ~358-359 months, median ~367-368 months).\n",
    "    * The standard deviations are slightly higher for unapproved loans, but the overall pattern is consistent.\n",
    "    * Boxplots show that the majority of values cluster tightly around the median, with a few extreme outliers.\n",
    "\n",
    "3. Correlation:\n",
    "    * There is an extremely weak positive correlation (0.0083) between Loan_Status and Loan_Amount_Term, suggesting almost no linear relationship.\n",
    "\n",
    "4. Missing Values:\n",
    "    * No missing values were observed, which simplifies preprocessing for this feature.\n",
    "\n",
    "\n",
    "Recommendations for Loan_Amount_Term:\n",
    "\n",
    "1. Feature Transformation:\n",
    "\n",
    "    * Since the feature has weak correlation and limited variability for Loan_Status, we can either:\n",
    "        * Leave it as-is (retain as numeric).\n",
    "        * Bin the values into broader categories (e.g., short-term, medium-term, long-term) to simplify the feature and potentially make it more interpretable.\n",
    "\n",
    "2. Handling Outliers:\n",
    "    * Outliers should be considered carefully. For example:\n",
    "        * Check for business logic around extremely small or large values (e.g., terms under 100 months or over 400 months).\n",
    "        * Remove or cap outliers if they are deemed unrealistic.\n",
    "\n",
    "3. Interaction Terms:\n",
    "    * Create interaction features, such as Loan_Amount_Term multiplied or divided by LoanAmount, to explore if repayment terms relative to the loan amount influence Loan_Status.\n",
    "\n",
    "4. Retention Decision:\n",
    "    * Retain this feature in the model as it might contribute marginally to prediction despite weak correlation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning Loan_Amount_Term\n",
    "bins = [0, 180, 360, 480]\n",
    "labels = ['Short-term (<180)', 'Medium-term (180-360)', 'Long-term (>360)']\n",
    "train['Loan_Term_Bins'] = pd.cut(train['Loan_Amount_Term'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# Distribution of Loan_Status across bins\n",
    "term_status_dist = train.groupby(['Loan_Term_Bins', 'Loan_Status'])['Loan_ID'].count().unstack()\n",
    "term_status_dist.plot(kind='bar', stacked=True, figsize=(10, 6), color=['lightblue', 'salmon'])\n",
    "plt.title('Distribution of Loan Status by Loan Term Bins')\n",
    "plt.xlabel('Loan Term Bins')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(['Not Approved', 'Approved'], title='Loan Status')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Loan_Status', y='Loan_Amount_Term', data=train, palette='coolwarm')\n",
    "plt.title('Boxplot of Loan Amount Term by Loan Status')\n",
    "plt.xlabel('Loan Status (0=Not Approved, 1=Approved)')\n",
    "plt.ylabel('Loan Amount Term')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Loan_Term_Ratio'] = train['LoanAmount'] / train['Loan_Amount_Term']\n",
    "sns.boxplot(x='Loan_Status', y='Loan_Term_Ratio', data=train, palette='viridis')\n",
    "plt.title('Loan Amount to Loan Term Ratio by Loan Status')\n",
    "plt.xlabel('Loan Status (0=Not Approved, 1=Approved)')\n",
    "plt.ylabel('Loan Amount / Loan Term Ratio')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_terms = train['Loan_Amount_Term'].value_counts().head(10)\n",
    "top_terms.plot(kind='bar', color='purple', figsize=(8, 5))\n",
    "plt.title('Top 10 Most Frequent Loan Terms')\n",
    "plt.xlabel('Loan Amount Term')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = train[['Loan_Amount_Term', 'LoanAmount', 'ApplicantIncome', 'CoapplicantIncome']].corr()\n",
    "sns.heatmap(correlations, annot=True, cmap='coolwarm', fmt='.2f', vmin=-1, vmax=1)\n",
    "plt.title('Correlation of Loan_Amount_Term with Other Numerical Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Observations:\n",
    "\n",
    "1. Distribution:\n",
    "    * The Loan_Amount_Term feature is highly skewed, with most loans concentrated around 360 months (30 years).\n",
    "    * A few shorter-term loans exist (12 to 180 months), but these are rare.\n",
    "\n",
    "2. Correlation:\n",
    "    * Loan_Amount_Term shows minimal correlation with Loan_Status (0.008), indicating limited direct influence on loan approval.\n",
    "\n",
    "3. Interaction with Other Numerical Features:\n",
    "    * Weak correlations exist between Loan_Amount_Term and other numerical features like LoanAmount (0.05) and ApplicantIncome (-0.04).\n",
    "    * The scatterplot of the Loan_Amount/Loan_Amount_Term ratio highlights outliers in loan term and amount.\n",
    "\n",
    "4. Most Frequent Terms:\n",
    "    * The top 10 most frequent loan terms are around 360 months, suggesting a standard term length for loans.\n",
    "\n",
    "5. Loan Term Bins:\n",
    "    * Binning into categories like \"Short-term (<180 months),\" \"Medium-term (180-360 months),\" and \"Long-term (>360 months)\" highlights that most loans fall under long-term loans.\n",
    "\n",
    "6. Loan Term Ratio:\n",
    "    * The ratio of LoanAmount to Loan_Amount_Term helps standardize loan amount comparisons and reveals potential outliers.\n",
    "\n",
    "Recommendations:\n",
    "\n",
    "1. Feature Engineering:\n",
    "    * Bin Loan Terms: Create categorical bins for loan terms (e.g., short, medium, long-term) to better capture patterns in loan approval.\n",
    "    * Loan Term Ratio: The ratio of LoanAmount to Loan_Amount_Term can be used as a normalized feature representing the loan amount spread over the term.\n",
    "\n",
    "2. Outlier Handling:\n",
    "    * Investigate outliers in both Loan_Amount_Term and the LoanAmount/Loan_Amount_Term ratio for potential data issues or anomalies.\n",
    "\n",
    "3. Interaction Features:\n",
    "    * Combine Loan_Amount_Term with features like LoanAmount or ApplicantIncome to derive meaningful interaction terms that could provide predictive value.\n",
    "4. Modeling Considerations:\n",
    "    * Due to the low correlation with Loan_Status, Loan_Amount_Term may not be a strong predictor. However, interaction features or bins could reveal hidden patterns.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Binning Loan_Amount_Term\n",
    "def bin_loan_term(term):\n",
    "    if term < 180:\n",
    "        return 'Short-term'\n",
    "    elif 180 <= term <= 360:\n",
    "        return 'Medium-term'\n",
    "    else:\n",
    "        return 'Long-term'\n",
    "\n",
    "train['Loan_Term_Bins'] = train['Loan_Amount_Term'].apply(bin_loan_term)\n",
    "\n",
    "# Visualize the distribution of bins\n",
    "bin_counts = train['Loan_Term_Bins'].value_counts()\n",
    "plt.bar(bin_counts.index, bin_counts.values, color='purple')\n",
    "plt.title(\"Loan Term Bins Distribution\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xlabel(\"Loan Term Bins\")\n",
    "plt.show()\n",
    "\n",
    "# Step 2: Creating the Loan Amount to Loan Term Ratio\n",
    "train['LoanAmount_Term_Ratio'] = train['LoanAmount'] / train['Loan_Amount_Term']\n",
    "train['LoanAmount_Term_Ratio'].fillna(0, inplace=True)  # Handle divisions by zero\n",
    "\n",
    "# Visualize the LoanAmount_Term_Ratio distribution\n",
    "sns.histplot(train['LoanAmount_Term_Ratio'], kde=True, color='blue')\n",
    "plt.title(\"Distribution of Loan Amount to Loan Term Ratio\")\n",
    "plt.xlabel(\"Loan Amount / Loan Term Ratio\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations from the Visuals:\n",
    "\n",
    "1. Loan Term Bins Distribution:\n",
    "    * The majority of loans fall under the \"Long-term\" category (>360 months).\n",
    "    * A significant proportion is in the \"Medium-term\" range (180â€“360 months), while \"Short-term\" loans (<180 months) are rare.\n",
    "\n",
    "2. Loan Amount to Loan Term Ratio Distribution:\n",
    "    * The ratio is heavily skewed to the left, with most values concentrated near zero. This suggests that for most loans, the amount is small relative to the term.\n",
    "\n",
    "Recommendations:\n",
    "\n",
    "1. Loan Term Bins:\n",
    "    * Retain Loan_Term_Bins as a categorical feature for the model, as it provides interpretable groupings of loan durations.\n",
    "    * Perform additional analysis to check if loan approval rates differ significantly across these bins.\n",
    "\n",
    "2. Loan Amount to Term Ratio:\n",
    "    * Consider applying a logarithmic transformation to normalize the skewness, or bin this feature into categories to simplify its use in the model.\n",
    "\n",
    "3. Further Analysis:\n",
    "    * Investigate how Loan_Term_Bins and LoanAmount_Term_Ratio correlate with Loan_Status to confirm their predictive value.\n",
    "    * Perform bivariate analysis to explore interactions between Loan_Term_Bins and other features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Loan Term Bins Distribution by Loan Status\n",
    "loan_term_bin_status = train.groupby(['Loan_Term_Bins', 'Loan_Status']).size().unstack()\n",
    "loan_term_bin_status.plot(kind='bar', stacked=True, figsize=(10, 6), colormap='coolwarm')\n",
    "plt.title('Loan Term Bins Distribution by Loan Status')\n",
    "plt.xlabel('Loan Term Bins')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(['Not Approved', 'Approved'], title='Loan Status')\n",
    "plt.show()\n",
    "\n",
    "# 2. Boxplot of LoanAmount_Term_Ratio by Loan Status\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='Loan_Status', y='LoanAmount_Term_Ratio', data=train, palette='Set2')\n",
    "plt.title('Loan Amount to Loan Term Ratio by Loan Status')\n",
    "plt.xlabel('Loan Status (0=Not Approved, 1=Approved)')\n",
    "plt.ylabel('Loan Amount / Loan Term Ratio')\n",
    "plt.show()\n",
    "\n",
    "# 3. Correlation Heatmap with Loan_Term_Bins (One-Hot Encoded)\n",
    "loan_term_bins_encoded = pd.get_dummies(train['Loan_Term_Bins'], drop_first=True)\n",
    "correlation_matrix = pd.concat([loan_term_bins_encoded, train[['LoanAmount_Term_Ratio', 'Loan_Status']]], axis=1).corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Heatmap Including Loan Term Features')\n",
    "plt.show()\n",
    "\n",
    "# 4. Interaction Analysis: Scatter Plot of LoanAmount_Term_Ratio vs ApplicantIncome\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='LoanAmount_Term_Ratio', y='ApplicantIncome', hue='Loan_Status', data=train, palette='viridis')\n",
    "plt.title('Interaction of Loan Amount/Term Ratio and Applicant Income')\n",
    "plt.xlabel('Loan Amount / Loan Term Ratio')\n",
    "plt.ylabel('Applicant Income')\n",
    "plt.legend(title='Loan Status', loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Findings:\n",
    "1. Correlation Heatmap:\n",
    "    * The Loan_Amount_Term and its derived features (bins and ratio) show weak correlations with Loan_Status. This aligns with earlier observations that loan term features individually may not be highly predictive.\n",
    "\n",
    "2. Loan Term Bins Distribution:\n",
    "    * A majority of loans fall under the \"Long-term\" category, followed by \"Medium-term.\" Short-term loans are rare.\n",
    "    * Loan approval rates are highest for long-term loans, though this could also reflect the distribution bias of data being skewed toward long-term loans.\n",
    "\n",
    "3. Loan Amount/Term Ratio:\n",
    "    * Most observations cluster near lower values of the ratio, with a few outliers reaching high values. Approved and non-approved loans appear similar when analyzed against the ratio.\n",
    "\n",
    "4. Applicant Income Interaction:\n",
    "    * When plotting Loan Amount/Term Ratio against Applicant Income, we notice that higher ratios often align with lower incomes, indicating that lower-income applicants might request higher amounts relative to their loan terms. This could influence loan decisions but needs further statistical testing.\n",
    "\n",
    "Recommendations:\n",
    "\n",
    "1. Feature Selection:\n",
    "    * Retain the binned categories (Loan Term Bins) as they introduce categorical information that might enhance the interpretability of models.\n",
    "    * Include the Loan Amount/Term Ratio as a numerical feature to capture the relative size of the loan to its term, which could provide more nuanced insights in modeling.\n",
    "\n",
    "2. Interaction Terms:\n",
    "    * Consider creating interaction terms between Loan Term Bins and other numerical features like Applicant Income or Loan Amount. These could highlight patterns that are not obvious in univariate analysis.\n",
    "\n",
    "3. Model Testing:\n",
    "    * Use these features in preliminary model testing to evaluate their predictive power. The weak correlations observed so far may not fully capture non-linear relationships that more complex models like XGBoost or Random Forest could uncover.\n",
    "\n",
    "4. Statistical Testing:\n",
    "    * Perform hypothesis tests to check if the mean of Loan Amount/Term Ratio significantly differs between approved and non-approved loans. This could validate its inclusion in the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction between Loan Term Bins and Applicant Income\n",
    "train['Interaction_LoanBins_Income'] = train['Loan_Term_Bins'].map({\n",
    "    'Short-term': 1, 'Medium-term': 2, 'Long-term': 3}) * train['ApplicantIncome']\n",
    "\n",
    "# Interaction between Loan Term Bins and Loan Amount\n",
    "train['Interaction_LoanBins_Amount'] = train['Loan_Term_Bins'].map({\n",
    "    'Short-term': 1, 'Medium-term': 2, 'Long-term': 3}) * train['LoanAmount']\n",
    "\n",
    "# Interaction between Loan Amount/Term Ratio and Applicant Income\n",
    "train['Interaction_Ratio_Income'] = train['LoanAmount_Term_Ratio'] * train['ApplicantIncome']\n",
    "\n",
    "# Summary statistics of interaction terms\n",
    "interaction_summary = train[['Interaction_LoanBins_Income', 'Interaction_LoanBins_Amount', 'Interaction_Ratio_Income']].describe()\n",
    "interaction_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# T-test for Loan Amount/Term Ratio between Loan Status groups\n",
    "approved_ratio = train.loc[train['Loan_Status'] == 1, 'LoanAmount_Term_Ratio']\n",
    "not_approved_ratio = train.loc[train['Loan_Status'] == 0, 'LoanAmount_Term_Ratio']\n",
    "\n",
    "# T-test\n",
    "t_stat, p_value = ttest_ind(approved_ratio, not_approved_ratio, nan_policy='omit')\n",
    "\n",
    "# Results for T-test\n",
    "{\n",
    "    't_stat': t_stat,\n",
    "    'p_value': p_value\n",
    "}\n",
    "\n",
    "# T-test for Interaction_Ratio_Income\n",
    "approved_ratio_income = train.loc[train['Loan_Status'] == 1, 'Interaction_Ratio_Income']\n",
    "not_approved_ratio_income = train.loc[train['Loan_Status'] == 0, 'Interaction_Ratio_Income']\n",
    "\n",
    "# T-test\n",
    "t_stat_income, p_value_income = ttest_ind(approved_ratio_income, not_approved_ratio_income, nan_policy='omit')\n",
    "\n",
    "# Results for Interaction_Ratio_Income T-test\n",
    "{\n",
    "    't_stat_income': t_stat_income,\n",
    "    'p_value_income': p_value_income\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of Interaction Terms\n",
    "\n",
    "1. Summary Statistics:\n",
    "    * The mean values for the interaction terms (Interaction_LoanBins_Income, Interaction_LoanBins_Amount, and Interaction_Ratio_Income) are quite large, especially for Interaction_LoanBins_Income (mean = ~20,521) and Interaction_Ratio_Income (mean = ~2,312).\n",
    "    * There is significant variability in the data as indicated by the standard deviation (std) values, particularly for Interaction_Ratio_Income (std = ~12,231).\n",
    "    * Extreme values are present (e.g., max = 243,000 for Interaction_LoanBins_Income and 497,068 for Interaction_Ratio_Income), which may indicate potential outliers.\n",
    "\n",
    "2. T-Test for Interaction_Ratio_Income:\n",
    "    * The t-statistic is -2.28, and the corresponding p-value is 0.0228.\n",
    "    * Since the p-value is below the commonly used significance level of 0.05, the interaction term Interaction_Ratio_Income is statistically significant. This suggests that there is a significant difference in the mean Interaction_Ratio_Income between the Loan_Status groups (Approved vs. Not Approved).\n",
    "\n",
    "Recommendations\n",
    "\n",
    "1. Keep Interaction_Ratio_Income:\n",
    "    * Given its statistical significance, it should be considered as a feature in the modeling phase.\n",
    "    * However, due to the presence of extreme values, you might consider scaling this feature (e.g., MinMaxScaler or StandardScaler) before using it in a model.\n",
    "\n",
    "2. Monitor Other Interaction Terms:\n",
    "    * While Interaction_LoanBins_Income and Interaction_LoanBins_Amount were not directly tested for significance, their variability and descriptive statistics suggest potential utility. These features can be included as candidates in the feature selection phase.\n",
    "\n",
    "3. Handle Outliers:\n",
    "    * The high maximum values for the interaction terms may cause issues in modeling. You could cap these extreme values (e.g., Winsorization) or transform the data further.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# List of interaction terms\n",
    "interaction_terms = ['Interaction_LoanBins_Income', 'Interaction_LoanBins_Amount', 'Interaction_Ratio_Income']\n",
    "\n",
    "# Set up the plot grid\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, feature in enumerate(interaction_terms, 1):\n",
    "    plt.subplot(3, 1, i)\n",
    "    sns.histplot(train[feature], kde=True, bins=30, color='purple')\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Apply log transformations (adding 1 to avoid log(0))\n",
    "train['Log_Interaction_LoanBins_Income'] = np.log1p(train['Interaction_LoanBins_Income'])\n",
    "train['Log_Interaction_LoanBins_Amount'] = np.log1p(train['Interaction_LoanBins_Amount'])\n",
    "train['Log_Interaction_Ratio_Income'] = np.log1p(train['Interaction_Ratio_Income'])\n",
    "\n",
    "# Visualize transformed features by Loan_Status\n",
    "interaction_features = [\n",
    "    'Log_Interaction_LoanBins_Income',\n",
    "    'Log_Interaction_LoanBins_Amount',\n",
    "    'Log_Interaction_Ratio_Income'\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "for i, feature in enumerate(interaction_features, 1):\n",
    "    plt.subplot(3, 1, i)\n",
    "    sns.boxplot(data=train, x='Loan_Status', y=feature, palette='pastel')\n",
    "    plt.title(f'{feature} Distribution by Loan_Status')\n",
    "    plt.xlabel('Loan Status (0=Not Approved, 1=Approved)')\n",
    "    plt.ylabel(feature)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "1. Log_Interaction_LoanBins_Income:\n",
    "    * Approved loans (Loan_Status = 1) tend to have a higher median and a wider range of values compared to not approved loans.\n",
    "    * There are a few outliers on both ends, but the overall distribution suggests this feature might have some predictive power.\n",
    "\n",
    "2. Log_Interaction_LoanBins_Amount:\n",
    "    * Similar to the income-based interaction, approved loans show higher median values and a broader range.\n",
    "    * The feature seems to differentiate well between the two loan status categories.\n",
    "\n",
    "3. Log_Interaction_Ratio_Income:\n",
    "    * This feature appears to show a strong difference between the approved and not approved loans, with approved loans having a higher median.\n",
    "    * The range of values for approved loans is more spread out, indicating potential variability.\n",
    "\n",
    "\n",
    "Recommendations:\n",
    "\n",
    "* Feature Importance Check: Use these transformed features in a preliminary model (like logistic regression or feature importance from XGBoost) to evaluate their significance.\n",
    "* Further Testing: Perform hypothesis testing (e.g., t-tests or Mann-Whitney U tests) to assess the statistical significance of the differences between loan status categories for these features.\n",
    "* Scaling: Since these features are skewed, ensure they are appropriately scaled when used in models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exploring the Credit_History feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Check for missing values and unique values\n",
    "print(\"Missing values in Credit_History:\", train['Credit_History'].isnull().sum())\n",
    "print(\"Unique values in Credit_History:\", train['Credit_History'].unique())\n",
    "\n",
    "# 2. Plot the distribution of Credit_History\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=train, x='Credit_History', palette='viridis')\n",
    "plt.title('Distribution of Credit_History')\n",
    "plt.xlabel('Credit History')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# 3. Analyze the relationship between Credit_History and Loan_Status\n",
    "credit_loan_status = train.groupby(['Credit_History', 'Loan_Status']).size().unstack()\n",
    "credit_loan_status.plot(kind='bar', stacked=True, figsize=(10, 6), colormap='viridis')\n",
    "plt.title('Loan Status by Credit History')\n",
    "plt.xlabel('Credit History')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(['Not Approved', 'Approved'], title='Loan Status')\n",
    "plt.show()\n",
    "\n",
    "# 4. Summary statistics for Credit_History with respect to Loan_Status\n",
    "summary_credit = train.groupby('Loan_Status')['Credit_History'].value_counts(normalize=True).unstack()\n",
    "print(\"Proportion of Credit_History by Loan_Status:\\n\", summary_credit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations \n",
    "\n",
    "1. Distribution of Credit_History:\n",
    "    * The vast majority of applicants have a Credit_History value of 1 (positive credit history).\n",
    "    * Only a small portion of the dataset has a Credit_History value of 0 (negative credit history).\n",
    "\n",
    "2. Relationship with Loan_Status:\n",
    "\n",
    "    * Applicants with a Credit_History value of 1 have a significantly higher likelihood of loan approval compared to those with a Credit_History of 0.\n",
    "    * The proportion of loan approvals (Loan_Status = 1) for those with Credit_History = 1 is approximately 92%, while only 8% of applicants with Credit_History = 0 are approved.\n",
    "    * The relationship between Credit_History and Loan_Status appears strong and could be a key predictive feature.\n",
    "\n",
    "Recommendations:\n",
    "\n",
    "1. Feature Importance:\n",
    "    * Since Credit_History shows a clear and strong relationship with Loan_Status, it should be retained as an essential feature in the model.\n",
    "    * Consider using this feature as a categorical variable with appropriate encoding (e.g., one-hot or binary).\n",
    "\n",
    "2. Interaction Features:\n",
    "    * Create interaction terms between Credit_History and numerical features (e.g., ApplicantIncome, LoanAmount) to explore if they add predictive power to the model.\n",
    "\n",
    "3. Handling Imbalances:\n",
    "    * If class imbalance exists (e.g., significantly fewer applicants with Credit_History = 0), sampling techniques or careful model evaluation metrics (like F1-score) should be considered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction with continuous features\n",
    "train['Interaction_CreditIncome'] = train['Credit_History'] * train['Total_Income']\n",
    "train['Interaction_CreditLoan'] = train['Credit_History'] * train['LoanAmount']\n",
    "\n",
    "# Interaction with categorical features\n",
    "train['Interaction_CreditProperty'] = train['Credit_History'].astype(str) + \"_\" + train['Property_Area'].astype(str)\n",
    "\n",
    "# Visualize distributions of continuous interactions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "sns.histplot(train['Interaction_CreditIncome'], bins=30, kde=True, ax=axes[0], color=\"purple\")\n",
    "axes[0].set_title(\"Distribution of Credit History Ã— Total Income\")\n",
    "sns.histplot(train['Interaction_CreditLoan'], bins=30, kde=True, ax=axes[1], color=\"green\")\n",
    "axes[1].set_title(\"Distribution of Credit History Ã— Loan Amount\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze categorical interaction\n",
    "credit_property_counts = train['Interaction_CreditProperty'].value_counts()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=credit_property_counts.index, y=credit_property_counts.values, palette=\"muted\")\n",
    "plt.title(\"Counts of Credit History Ã— Property Area Interactions\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical testing (example for Interaction_CreditIncome)\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "group_0 = train[train['Loan_Status'] == 0]['Interaction_CreditIncome']\n",
    "group_1 = train[train['Loan_Status'] == 1]['Interaction_CreditIncome']\n",
    "t_stat, p_value = ttest_ind(group_0, group_1, equal_var=False)\n",
    "\n",
    "# Output the t-test results\n",
    "print(f\"T-statistic for Interaction_CreditIncome: {t_stat}\")\n",
    "print(f\"P-value for Interaction_CreditIncome: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "1. Distributions of Continuous Interactions:\n",
    "    * The interaction between Credit_History and Total_Income shows a bimodal distribution, likely driven by the binary nature of Credit_History (0 or 1).\n",
    "    * The interaction between Credit_History and LoanAmount is highly right-skewed, with most values concentrated near zero due to smaller loan amounts.\n",
    "\n",
    "2. Counts of Credit_History Ã— Property Area Interaction:\n",
    "    * The most frequent interaction categories are 1_Urban, 1_Semiurban, and 1_Rural, corresponding to Credit_History = 1 and different Property_Area values.\n",
    "    * Interactions involving Credit_History = 0 are significantly fewer, reflecting the imbalanced distribution of the Credit_History feature.\n",
    "\n",
    "3. Statistical Test Results:\n",
    "    * For the Interaction_CreditIncome feature, the t-test between approved (Loan_Status = 1) and not approved (Loan_Status = 0) loans shows no significant difference (p-value = 0.891), indicating that this interaction may not contribute much to differentiating loan approval status.\n",
    "\n",
    "Recommendations:\n",
    "\n",
    "1. Continuous Interactions:\n",
    "    * The Interaction_CreditLoan feature might be more promising for capturing variability due to its distribution. Perform further statistical testing to confirm its significance.\n",
    "\n",
    "2. Categorical Interaction:\n",
    "    * The Interaction_CreditProperty feature could capture meaningful trends when analyzed using a model that can handle categorical variables effectively, such as tree-based algorithms or embedding techniques.\n",
    "\n",
    "Next Steps:\n",
    "\n",
    "Perform similar analyses for the remaining features, including Property_Area and Total_Income.\n",
    "Include interaction terms in feature selection pipelines to identify their potential contribution to predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exploring the Property_Area feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Basic Exploration\n",
    "print(f\"Missing values in Property_Area: {train['Property_Area'].isnull().sum()}\")\n",
    "print(f\"Unique values in Property_Area: {train['Property_Area'].unique()}\")\n",
    "proportion_area = pd.crosstab(train['Property_Area'], train['Loan_Status'], normalize='index')\n",
    "print(\"\\nProportion of Property_Area by Loan_Status:\")\n",
    "print(proportion_area)\n",
    "\n",
    "# 2. Visualize Distributions\n",
    "fig, axs = plt.subplots(2, 1, figsize=(8, 12))\n",
    "sns.countplot(data=train, x='Property_Area', ax=axs[0], palette='viridis')\n",
    "axs[0].set_title(\"Overall Distribution of Property_Area\")\n",
    "axs[0].set_xlabel(\"Property Area\")\n",
    "axs[0].set_ylabel(\"Count\")\n",
    "\n",
    "sns.countplot(data=train, x='Property_Area', hue='Loan_Status', ax=axs[1], palette='viridis')\n",
    "axs[1].set_title(\"Loan Status by Property_Area\")\n",
    "axs[1].set_xlabel(\"Property Area\")\n",
    "axs[1].set_ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Interaction Features\n",
    "train['Interaction_PropertyIncome'] = train['Property_Area'].astype(str) + \"_\" + train['ApplicantIncome'].astype(str)\n",
    "train['Interaction_PropertyCoapplicant'] = train['Property_Area'].astype(str) + \"_\" + train['CoapplicantIncome'].astype(str)\n",
    "train['Interaction_PropertyLoan'] = train['Property_Area'].astype(str) + \"_\" + train['LoanAmount'].astype(str)\n",
    "\n",
    "# 4. Statistical Testing for Interaction_PropertyIncome (example)\n",
    "property_income = train[['Interaction_PropertyIncome', 'Loan_Status']].copy()\n",
    "property_income['PropertyArea_Num'] = train['Property_Area'].factorize()[0]\n",
    "property_income['Log_ApplicantIncome'] = np.log1p(train['ApplicantIncome'])\n",
    "t_stat, p_value = ttest_ind(\n",
    "    property_income[property_income['Loan_Status'] == 1]['Log_ApplicantIncome'],\n",
    "    property_income[property_income['Loan_Status'] == 0]['Log_ApplicantIncome'],\n",
    "    equal_var=False\n",
    ")\n",
    "print(f\"T-statistic for Interaction_PropertyIncome: {t_stat}\")\n",
    "print(f\"P-value for Interaction_PropertyIncome: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations from Property_Area Analysis:\n",
    "\n",
    "1. Overall Distribution:\n",
    "    * The Property_Area feature contains three categories: 0, 1, and 2.\n",
    "    * The count distribution shows that categories 1 and 2 have a higher frequency than 0.\n",
    "\n",
    "2. Loan Status by Property Area:\n",
    "    * Across all categories, the proportion of loan approvals (Loan_Status = 1) is higher than rejections.\n",
    "    * The highest approval rate is observed in Property_Area = 2 (84.48%), followed by Property_Area = 1 (82.95%), and then Property_Area = 0 (81.79%).\n",
    "\n",
    "3. Interaction with Income:\n",
    "    * A t-test for the interaction between Property_Area and ApplicantIncome against Loan_Status yielded:\n",
    "        * T-statistic: 0.554\n",
    "        * P-value: 0.580\n",
    "    * The p-value is greater than 0.05, indicating no significant difference in applicant income distributions across loan statuses for different property areas.\n",
    "\n",
    "Recommendations for the Property_Area Feature:\n",
    "\n",
    "1. Encoding:\n",
    "    * Since Property_Area is categorical, it can be encoded using one-hot encoding or ordinal encoding based on model preference.\n",
    "\n",
    "2. Interactions:\n",
    "    * The interaction with income does not show statistical significance. However, it might still contribute to non-linear patterns, so consider keeping the interactions (Property_Area Ã— Income or Property_Area Ã— LoanAmount) for further modeling.\n",
    "    \n",
    "3. Feature Usefulness:\n",
    "    * Property_Area has a strong association with loan status, as evident from the approval proportions. It should be retained as an important feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interaction features\n",
    "train['Interaction_PropertyLoan'] = train['Property_Area'] * train['LoanAmount']\n",
    "train['Interaction_PropertyCredit'] = train['Property_Area'] * train['Credit_History']\n",
    "\n",
    "# Summary statistics for the interactions\n",
    "interaction_summary = train[['Interaction_PropertyLoan', 'Interaction_PropertyCredit']].describe()\n",
    "\n",
    "# Visualization of interaction distributions\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot for Interaction_PropertyLoan\n",
    "plt.subplot(2, 1, 1)\n",
    "sns.histplot(train['Interaction_PropertyLoan'], kde=True, color='purple', bins=30)\n",
    "plt.title('Distribution of Property_Area Ã— LoanAmount Interaction')\n",
    "plt.xlabel('Interaction_PropertyLoan')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Plot for Interaction_PropertyCredit\n",
    "plt.subplot(2, 1, 2)\n",
    "sns.histplot(train['Interaction_PropertyCredit'], kde=True, color='green', bins=30)\n",
    "plt.title('Distribution of Property_Area Ã— Credit_History Interaction')\n",
    "plt.xlabel('Interaction_PropertyCredit')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Group by Loan_Status for additional analysis\n",
    "grouped_interactions = train.groupby('Loan_Status')[['Interaction_PropertyLoan', 'Interaction_PropertyCredit']].mean()\n",
    "\n",
    "# Statistical testing for interaction significance\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Interaction_PropertyLoan significance\n",
    "t_stat_property_loan, p_value_property_loan = ttest_ind(\n",
    "    train[train['Loan_Status'] == 1]['Interaction_PropertyLoan'],\n",
    "    train[train['Loan_Status'] == 0]['Interaction_PropertyLoan']\n",
    ")\n",
    "\n",
    "# Interaction_PropertyCredit significance\n",
    "t_stat_property_credit, p_value_property_credit = ttest_ind(\n",
    "    train[train['Loan_Status'] == 1]['Interaction_PropertyCredit'],\n",
    "    train[train['Loan_Status'] == 0]['Interaction_PropertyCredit']\n",
    ")\n",
    "\n",
    "# Print results\n",
    "interaction_tests = {\n",
    "    'Interaction_PropertyLoan': {'t_stat': t_stat_property_loan, 'p_value': p_value_property_loan},\n",
    "    'Interaction_PropertyCredit': {'t_stat': t_stat_property_credit, 'p_value': p_value_property_credit}\n",
    "}\n",
    "\n",
    "print(\"Interaction Summary Statistics:\\n\", interaction_summary)\n",
    "print(\"\\nGrouped Interactions by Loan_Status:\\n\", grouped_interactions)\n",
    "print(\"\\nInteraction Statistical Tests:\\n\", interaction_tests)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of Interaction Features for Property_Area\n",
    "\n",
    "1. Distribution Insights:\n",
    "    * Interaction_PropertyLoan:\n",
    "        * Skewed towards smaller values, with most interactions concentrated below 200.\n",
    "        * Maximum value is 1400, indicating some extreme cases.\n",
    "\n",
    "    * Interaction_PropertyCredit:\n",
    "        * Bimodal distribution reflecting the binary nature of Credit_History and discrete Property_Area values.\n",
    "        * Most values are concentrated at 0, 1, or 2.\n",
    "\n",
    "2. Summary Statistics:\n",
    "    * Interaction_PropertyLoan has a mean of 114.86, with a wide range (0â€“1400) and high variability (std = 160.01).\n",
    "    * Interaction_PropertyCredit is mostly clustered around its discrete levels (mean = 1.09, std = 0.77).\n",
    "\n",
    "3. Grouped Insights by Loan_Status:\n",
    "    * For Interaction_PropertyLoan, loans that are approved (1) have slightly higher mean values (116.35 vs. 107.42).\n",
    "    * For Interaction_PropertyCredit, approved loans also have a slightly higher mean (1.10 vs. 1.04).\n",
    "\n",
    "4. Statistical Significance:\n",
    "    * Interaction_PropertyLoan:\n",
    "        * p_value = 0.1098, indicating no statistically significant difference between groups.\n",
    "    * Interaction_PropertyCredit:\n",
    "        * p_value = 0.0478, indicating a statistically significant difference between approved and non-approved loans.\n",
    "\n",
    "Recommendations:\n",
    "\n",
    "* Interaction_PropertyLoan: This feature may not add substantial predictive power due to the lack of statistical significance. It can be considered for removal unless further feature engineering or transformations improve its relevance.\n",
    "* Interaction_PropertyCredit: Given its statistical significance, this feature should be retained as it demonstrates a meaningful relationship with the target variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exploring Total_Income feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Basic Statistics and Distribution\n",
    "print(\"Summary statistics for Total_Income:\")\n",
    "print(train[\"Total_Income\"].describe())\n",
    "\n",
    "# Plot the distribution of Total_Income\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(train[\"Total_Income\"], kde=True, color=\"purple\", bins=30)\n",
    "plt.title(\"Distribution of Total_Income\")\n",
    "plt.xlabel(\"Total_Income\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# 2. Target-Based Distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(x=\"Loan_Status\", y=\"Total_Income\", data=train, palette=\"pastel\")\n",
    "plt.title(\"Total_Income Distribution by Loan_Status\")\n",
    "plt.xlabel(\"Loan_Status\")\n",
    "plt.ylabel(\"Total_Income\")\n",
    "plt.show()\n",
    "\n",
    "# 3. Log Transformation\n",
    "train[\"Log_Total_Income\"] = np.log1p(train[\"Total_Income\"])\n",
    "\n",
    "# Plot the distribution of log-transformed Total_Income\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(train[\"Log_Total_Income\"], kde=True, color=\"green\", bins=30)\n",
    "plt.title(\"Distribution of Log-Transformed Total_Income\")\n",
    "plt.xlabel(\"Log_Total_Income\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# Boxplot for log-transformed Total_Income grouped by Loan_Status\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(x=\"Loan_Status\", y=\"Log_Total_Income\", data=train, palette=\"muted\")\n",
    "plt.title(\"Log_Total_Income Distribution by Loan_Status\")\n",
    "plt.xlabel(\"Loan_Status\")\n",
    "plt.ylabel(\"Log_Total_Income\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Observations:\n",
    "\n",
    "1. Distribution:\n",
    "    * The feature has a right-skewed distribution, as evident from both the raw and log-transformed histograms.\n",
    "    * Log transformation significantly normalizes the distribution, which can be useful for modeling.\n",
    "\n",
    "2. Relationship with Loan_Status:\n",
    "    * Higher-income applicants (both raw and log-transformed) appear to have higher loan approval rates.\n",
    "    * Approved applicants tend to have slightly higher median total income compared to not-approved applicants.\n",
    "\n",
    "3. Statistics:\n",
    "    * Total income ranges from 1963 to 22,500, with a median of 6000.\n",
    "    * The log-transformed values are more centered, making them potentially better for feature representation in models.\n",
    "\n",
    "Next Steps:\n",
    "\n",
    "1. Interactions:\n",
    "    * Create interaction terms for Total_Income with other features, such as:\n",
    "        * Loan_Amount (e.g., TotalIncome_to_LoanAmount_Ratio).\n",
    "        * Credit_History (e.g., CreditHistory_TotalIncome).\n",
    "        * Loan_Amount_Term (e.g., Income_Per_LoanTerm).\n",
    "    * Visualize these interactions and assess their importance.\n",
    "\n",
    "2. Feature Engineering:\n",
    "    * Decide whether to use the raw or log-transformed feature based on correlation and model performance.\n",
    "\n",
    "3. Statistical Testing:\n",
    "    * Perform statistical tests (e.g., t-test or ANOVA) to determine if Total_Income significantly impacts Loan_Status.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interaction terms\n",
    "train['TotalIncome_to_LoanAmount_Ratio'] = train['Total_Income'] / (train['LoanAmount'] + 1e-6)\n",
    "train['Income_Per_LoanTerm'] = train['Total_Income'] / (train['Loan_Amount_Term'] + 1e-6)\n",
    "train['CreditHistory_TotalIncome'] = train['Credit_History'] * train['Total_Income']\n",
    "\n",
    "# Visualize distributions of interactions\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10, 15))\n",
    "sns.histplot(train['TotalIncome_to_LoanAmount_Ratio'], kde=True, ax=axes[0], color=\"purple\")\n",
    "axes[0].set_title(\"Distribution of Total Income to Loan Amount Ratio\")\n",
    "sns.histplot(train['Income_Per_LoanTerm'], kde=True, ax=axes[1], color=\"blue\")\n",
    "axes[1].set_title(\"Distribution of Income Per Loan Term\")\n",
    "sns.histplot(train['CreditHistory_TotalIncome'], kde=True, ax=axes[2], color=\"green\")\n",
    "axes[2].set_title(\"Distribution of Credit History Ã— Total Income\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Boxplots by Loan_Status\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10, 15))\n",
    "sns.boxplot(data=train, x='Loan_Status', y='TotalIncome_to_LoanAmount_Ratio', ax=axes[0], palette=\"Set2\")\n",
    "axes[0].set_title(\"Total Income to Loan Amount Ratio by Loan Status\")\n",
    "sns.boxplot(data=train, x='Loan_Status', y='Income_Per_LoanTerm', ax=axes[1], palette=\"Set2\")\n",
    "axes[1].set_title(\"Income Per Loan Term by Loan Status\")\n",
    "sns.boxplot(data=train, x='Loan_Status', y='CreditHistory_TotalIncome', ax=axes[2], palette=\"Set2\")\n",
    "axes[2].set_title(\"Credit History Ã— Total Income by Loan Status\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Perform statistical tests\n",
    "stats_results = {}\n",
    "for col in ['TotalIncome_to_LoanAmount_Ratio', 'Income_Per_LoanTerm', 'CreditHistory_TotalIncome']:\n",
    "    t_stat, p_value = ttest_ind(\n",
    "        train[train['Loan_Status'] == 1][col],\n",
    "        train[train['Loan_Status'] == 0][col],\n",
    "        nan_policy='omit'\n",
    "    )\n",
    "    stats_results[col] = {'t_stat': t_stat, 'p_value': p_value}\n",
    "\n",
    "# Display statistical test results\n",
    "print(\"Statistical Test Results for Interactions:\")\n",
    "for key, value in stats_results.items():\n",
    "    print(f\"{key}: T-stat = {value['t_stat']:.4f}, P-value = {value['p_value']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: \n",
    "\n",
    "1. TotalIncome_to_LoanAmount_Ratio\n",
    "    * Visual Interpretation:\n",
    "        * The distribution shows a right-skewed pattern, with most values concentrated in the lower range.\n",
    "        * When stratified by Loan_Status, both approved (1) and not approved (0) loans show similar distributions, but the approved group seems to have slightly higher ratios in its upper range.\n",
    "    * Statistical Test:\n",
    "        * T-statistic: -0.7150, P-value: 0.4746\n",
    "        * Interpretation: There is no statistically significant difference in the TotalIncome_to_LoanAmount_Ratio between approved and not approved loans (p > 0.05). This suggests that this ratio might not strongly influence the loan approval outcome.\n",
    "\n",
    "2. Income_Per_LoanTerm\n",
    "    * Visual Interpretation:\n",
    "        * The boxplot shows most values concentrated at the lower end, with some significant outliers in both loan statuses.\n",
    "        * Approved loans (1) have a slightly higher median Income_Per_LoanTerm compared to not approved loans (0), but the difference is not visually striking.\n",
    "    * Statistical Test:\n",
    "        * T-statistic: -1.8450, P-value: 0.0651\n",
    "        * Interpretation: The difference in Income_Per_LoanTerm between approved and not approved loans is borderline significant (p â‰ˆ 0.065). While not statistically significant at the 0.05 level, the near-significance suggests this feature might contribute to the loan approval process, especially in models that capture non-linear relationships.\n",
    "\n",
    "3. CreditHistory_TotalIncome\n",
    "    * Visual Interpretation:\n",
    "        * The distribution highlights peaks corresponding to the primary income categories in the dataset, showing a bimodal pattern.\n",
    "        * Boxplots reveal that the distribution of CreditHistory_TotalIncome is fairly similar for both loan statuses, with approved loans showing slightly higher median values.\n",
    "    * Statistical Test:\n",
    "        * T-statistic: 0.1369, P-value: 0.8911\n",
    "        * Interpretation: There is no statistically significant difference in CreditHistory_TotalIncome between approved and not approved loans (p > 0.05). This suggests that this feature, in isolation, might not strongly differentiate the loan approval status.\n",
    "\n",
    "Summary of Insights:\n",
    "* Among the three interactive features analyzed, Income_Per_LoanTerm appears to show the most potential for distinguishing between loan statuses, albeit borderline significant.\n",
    "* TotalIncome_to_LoanAmount_Ratio and CreditHistory_TotalIncome do not exhibit significant differences between loan statuses, indicating they might have limited direct influence on loan approval. However, these features could still be useful in combination with others in a model that captures complex patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering & Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Set ID column as Index and Drop Loan_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set ID column as the index\n",
    "train.set_index('ID', inplace=True)\n",
    "test.set_index('ID', inplace=True)\n",
    "\n",
    "# Drop the Loan_ID column\n",
    "train.drop('Loan_ID', axis=1, inplace=True)\n",
    "test.drop('Loan_ID', axis=1, inplace=True)\n",
    "\n",
    "# Fix for the Dependents column\n",
    "# Replace '3+' with 3 and ensure Dependents is numeric\n",
    "train['Dependents'] = train['Dependents'].replace('3+', 3).astype(int)\n",
    "test['Dependents'] = test['Dependents'].replace('3+', 3).astype(int)\n",
    "\n",
    "# Create Dependents_Grouped by grouping 2 and 3 into a single category (2+)\n",
    "train['Dependents_Grouped'] = train['Dependents'].apply(lambda x: 2 if x >= 2 else x)\n",
    "test['Dependents_Grouped'] = test['Dependents'].apply(lambda x: 2 if x >= 2 else x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: Log Transformation\n",
    "We will apply log transformation to continuous features with skewed distributions in both train and test. Features like ApplicantIncome, CoapplicantIncome, LoanAmount, and Total_Income are good candidates for log transformation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features for log transformation\n",
    "log_features = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Total_Income']\n",
    "\n",
    "# Apply log1p (log(x+1)) transformation to handle zero or negative values\n",
    "for feature in log_features:\n",
    "    train[f'Log_{feature}'] = np.log1p(train[feature])\n",
    "    test[f'Log_{feature}'] = np.log1p(test[feature])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: Interaction Features\n",
    "Create the interaction features we identified during analysis. Apply the same transformations to both datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction features\n",
    "train['Loan_Amount_to_Loan_Term_Ratio'] = train['LoanAmount'] / train['Loan_Amount_Term']\n",
    "test['Loan_Amount_to_Loan_Term_Ratio'] = test['LoanAmount'] / test['Loan_Amount_Term']\n",
    "\n",
    "train['CreditHistory_TotalIncome'] = train['Credit_History'] * train['Total_Income']\n",
    "test['CreditHistory_TotalIncome'] = test['Credit_History'] * test['Total_Income']\n",
    "\n",
    "train['Income_Per_LoanTerm'] = train['Total_Income'] / train['Loan_Amount_Term']\n",
    "test['Income_Per_LoanTerm'] = test['Total_Income'] / test['Loan_Amount_Term']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4: Loan Term Binning\n",
    "Create categorical bins for Loan_Amount_Term in both datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bins and labels\n",
    "bins = [0, 180, 360, float('inf')]\n",
    "labels = ['Short-term', 'Medium-term', 'Long-term']\n",
    "\n",
    "# Apply binning\n",
    "train['Loan_Term_Bin'] = pd.cut(train['Loan_Amount_Term'], bins=bins, labels=labels)\n",
    "test['Loan_Term_Bin'] = pd.cut(test['Loan_Amount_Term'], bins=bins, labels=labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 5: Categorical Encoding\n",
    "Convert categorical variables into numeric format using one-hot or label encoding. Apply the encoding to both datasets consistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define categorical features\n",
    "categorical_features = ['Gender', 'Married', 'Education', 'Self_Employed', 'Property_Area', 'Loan_Term_Bin']\n",
    "\n",
    "# One-hot encode the features\n",
    "train = pd.get_dummies(train, columns=categorical_features, drop_first=True)\n",
    "test = pd.get_dummies(test, columns=categorical_features, drop_first=True)\n",
    "\n",
    "# Ensure both datasets have the same columns\n",
    "missing_cols = set(train.columns) - set(test.columns)\n",
    "for col in missing_cols:\n",
    "    test[col] = 0  # Add missing columns to the test dataset\n",
    "\n",
    "# Align train and test datasets\n",
    "test = test[train.columns.drop('Loan_Status')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 6: Scaling\n",
    "Normalize continuous features using Min-Max scaling or Standard scaling. Scaling should be fit on the train set and applied to both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Continuous features to scale\n",
    "continuous_features = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Total_Income', \n",
    "                       'Loan_Amount_to_Loan_Term_Ratio', 'CreditHistory_TotalIncome', 'Income_Per_LoanTerm']\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit scaler on train data\n",
    "scaler.fit(train[continuous_features])\n",
    "\n",
    "# Transform both train and test\n",
    "train[continuous_features] = scaler.transform(train[continuous_features])\n",
    "test[continuous_features] = scaler.transform(test[continuous_features])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 7: Convert Booleans to Integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of boolean columns to convert\n",
    "boolean_columns = ['Gender_1',\n",
    "       'Married_1', 'Education_1', 'Self_Employed_1', 'Property_Area_1',\n",
    "       'Property_Area_2', 'Loan_Term_Bin_Medium-term',\n",
    "       'Loan_Term_Bin_Long-term']\n",
    "\n",
    "# Convert each column in the list from True/False to 1/0\n",
    "for col in boolean_columns:\n",
    "    train[col] = train[col].astype(int)\n",
    "    test[col] = test[col].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 8: Save Processed Data\n",
    "Save the transformed datasets for use in modeling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed datasets\n",
    "train.to_csv('data/Train_Processed.csv', index=True)\n",
    "test.to_csv('data/Test_Processed.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model & Feature Engineering & Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Data\n",
    "Since the Loan_Status column is the target in the train dataset, we need to split it into features (X_train) and the target (y_train). Additionally, we would want to create a validation set for model evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/Train_Processed.csv', index_col='ID')\n",
    "test = pd.read_csv('data/Test_Processed.csv', index_col='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.corr()['Loan_Status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your dataset\n",
    "X = train.drop(columns=['Loan_Status'])\n",
    "y = train['Loan_Status']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Addressing Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check the class distribution after SMOTE\n",
    "print(\"Class distribution after SMOTE:\")\n",
    "print(y_train_res.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Initialize LightGBM with default parameters\n",
    "lgbm_model = LGBMClassifier(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "lgbm_model.fit(\n",
    "    X_train_res, \n",
    "    y_train_res,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric='auc',\n",
    "    callbacks=[early_stopping(stopping_rounds=50, verbose=-1)]\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "y_val_pred = lgbm_model.predict(X_val)\n",
    "y_val_prob = lgbm_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_val_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "print(\"\\nROC-AUC Score:\")\n",
    "print(roc_auc_score(y_val, y_val_prob))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize a random forest model for feature importance\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(train.drop('Loan_Status', axis=1), train['Loan_Status'])\n",
    "\n",
    "# Extract feature importances\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': train.drop('Loan_Status', axis=1).columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importances['Feature'], feature_importances['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Re evaluating Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Plot numerical features against the target\n",
    "num_features = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Total_Income']\n",
    "for feature in num_features:\n",
    "    sns.boxplot(x=train['Loan_Status'], y=train[feature])\n",
    "    plt.title(f\"{feature} vs Loan_Status\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Evaluate interaction terms\n",
    "interaction_features = ['Income_Per_LoanTerm', 'CreditHistory_TotalIncome']\n",
    "X_interactions = train[interaction_features]\n",
    "y = train['Loan_Status']\n",
    "\n",
    "# Train a logistic regression to see if interactions add value\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_interactions, y)\n",
    "predictions = model.predict_proba(X_interactions)[:, 1]\n",
    "auc = roc_auc_score(y, predictions)\n",
    "print(f\"ROC-AUC for Interaction Features: {auc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Addressing Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define percentile caps for outliers\n",
    "caps = {\n",
    "    'ApplicantIncome': 0.99,\n",
    "    'CoapplicantIncome': 0.99,\n",
    "    'LoanAmount': 0.99,\n",
    "    'Total_Income': 0.99,\n",
    "}\n",
    "\n",
    "# Apply capping\n",
    "for col, cap in caps.items():\n",
    "    upper_limit = train[col].quantile(cap)\n",
    "    train[col] = np.clip(train[col], None, upper_limit)\n",
    "    test[col] = np.clip(test[col], None, upper_limit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Enhance Interaction Features\n",
    "Action:\n",
    "* Introduce and refine interaction terms based on domain knowledge and observed patterns.\n",
    "\n",
    "Features to Consider:\n",
    "\n",
    "1. Income Ratios:\n",
    "    * ApplicantIncome / CoapplicantIncome\n",
    "    * ApplicantIncome / LoanAmount\n",
    "    * CoapplicantIncome / LoanAmount\n",
    "\n",
    "2. Loan Term Ratios:\n",
    "    * LoanAmount / Loan_Amount_Term\n",
    "    * Total_Income / Loan_Amount_Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interaction features\n",
    "train['ApplicantIncome_to_CoapplicantIncome'] = train['ApplicantIncome'] / (train['CoapplicantIncome'] + 1)\n",
    "train['ApplicantIncome_to_LoanAmount'] = train['ApplicantIncome'] / (train['LoanAmount'] + 1)\n",
    "train['CoapplicantIncome_to_LoanAmount'] = train['CoapplicantIncome'] / (train['LoanAmount'] + 1)\n",
    "\n",
    "train['LoanAmount_to_LoanTerm'] = train['LoanAmount'] / train['Loan_Amount_Term']\n",
    "train['Total_Income_to_LoanTerm'] = train['Total_Income'] / train['Loan_Amount_Term']\n",
    "\n",
    "# Apply the same for the test set\n",
    "test['ApplicantIncome_to_CoapplicantIncome'] = test['ApplicantIncome'] / (test['CoapplicantIncome'] + 1)\n",
    "test['ApplicantIncome_to_LoanAmount'] = test['ApplicantIncome'] / (test['LoanAmount'] + 1)\n",
    "test['CoapplicantIncome_to_LoanAmount'] = test['CoapplicantIncome'] / (test['LoanAmount'] + 1)\n",
    "\n",
    "test['LoanAmount_to_LoanTerm'] = test['LoanAmount'] / test['Loan_Amount_Term']\n",
    "test['Total_Income_to_LoanTerm'] = test['Total_Income'] / test['Loan_Amount_Term']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rationale**: These features explore relationships between income, loan amount, and loan terms that directly influence loan approvals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reviewed Engineering Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload original dataset if needed\n",
    "train = pd.read_csv('data/Train.csv')\n",
    "test = pd.read_csv('data/Test.csv')\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reload original dataset if needed\n",
    "# train = pd.read_csv('data/Train.csv')\n",
    "# test = pd.read_csv('data/Test.csv')\n",
    "\n",
    "# # Set ID column as index\n",
    "# train.set_index('ID', inplace=True)\n",
    "# test.set_index('ID', inplace=True)\n",
    "\n",
    "# # Drop Loan_ID column\n",
    "# train.drop('Loan_ID', axis=1, inplace=True)\n",
    "# test.drop('Loan_ID', axis=1, inplace=True)\n",
    "\n",
    "# # Handle Dependents column\n",
    "# train['Dependents'] = train['Dependents'].replace('3+', 3).astype(int)\n",
    "# test['Dependents'] = test['Dependents'].replace('3+', 3).astype(int)\n",
    "# train['Dependents_Grouped'] = train['Dependents'].apply(lambda x: 2 if x >= 2 else x)\n",
    "# test['Dependents_Grouped'] = test['Dependents'].apply(lambda x: 2 if x >= 2 else x)\n",
    "\n",
    "# # Calculate Total_Income (if not already done)\n",
    "# train['Total_Income'] = train['ApplicantIncome'] + train['CoapplicantIncome'] + train['Total_Income']\n",
    "# test['Total_Income'] = test['ApplicantIncome'] + test['CoapplicantIncome'] + test['Total_Income']\n",
    "\n",
    "# # Log transform features\n",
    "# log_features = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Total_Income']\n",
    "# for feature in log_features:\n",
    "#     train[f'Log_{feature}'] = np.log1p(train[feature])\n",
    "#     test[f'Log_{feature}'] = np.log1p(test[feature])\n",
    "\n",
    "# # Interaction features\n",
    "# train['Loan_Amount_to_Loan_Term_Ratio'] = train['LoanAmount'] / train['Loan_Amount_Term']\n",
    "# test['Loan_Amount_to_Loan_Term_Ratio'] = test['LoanAmount'] / test['Loan_Amount_Term']\n",
    "\n",
    "# train['CreditHistory_TotalIncome'] = train['Credit_History'] * train['Total_Income']\n",
    "# test['CreditHistory_TotalIncome'] = test['Credit_History'] * test['Total_Income']\n",
    "\n",
    "# train['Income_Per_LoanTerm'] = train['Total_Income'] / train['Loan_Amount_Term']\n",
    "# test['Income_Per_LoanTerm'] = test['Total_Income'] / test['Loan_Amount_Term']\n",
    "\n",
    "# # Add missing interaction features\n",
    "# train['ApplicantIncome_Squared'] = train['ApplicantIncome'] ** 2\n",
    "# test['ApplicantIncome_Squared'] = test['ApplicantIncome'] ** 2\n",
    "\n",
    "# train['LoanAmount_TotalIncome_Interaction'] = train['LoanAmount'] * train['Total_Income']\n",
    "# test['LoanAmount_TotalIncome_Interaction'] = test['LoanAmount'] * test['Total_Income']\n",
    "\n",
    "# # Polynomial features\n",
    "# poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "# train_poly_features = poly.fit_transform(train[['ApplicantIncome', 'LoanAmount']])\n",
    "# test_poly_features = poly.transform(test[['ApplicantIncome', 'LoanAmount']])\n",
    "\n",
    "# poly_feature_names = poly.get_feature_names_out(['ApplicantIncome', 'LoanAmount'])\n",
    "# train[poly_feature_names] = train_poly_features\n",
    "# test[poly_feature_names] = test_poly_features\n",
    "\n",
    "# # Binning Loan_Amount_Term\n",
    "# bins = [0, 180, 360, float('inf')]\n",
    "# labels = ['Short-term', 'Medium-term', 'Long-term']\n",
    "# train['Loan_Term_Bin'] = pd.cut(train['Loan_Amount_Term'], bins=bins, labels=labels)\n",
    "# test['Loan_Term_Bin'] = pd.cut(test['Loan_Amount_Term'], bins=bins, labels=labels)\n",
    "\n",
    "# # One-hot encode categorical features\n",
    "# categorical_features = ['Gender', 'Married', 'Education', 'Self_Employed', 'Property_Area', 'Loan_Term_Bin']\n",
    "# train = pd.get_dummies(train, columns=categorical_features, drop_first=True)\n",
    "# test = pd.get_dummies(test, columns=categorical_features, drop_first=True)\n",
    "\n",
    "# # Align columns\n",
    "# test = test.reindex(columns=train.columns.drop('Loan_Status'), fill_value=0)\n",
    "\n",
    "# # Continuous features\n",
    "# continuous_features = [\n",
    "#     'ApplicantIncome', \n",
    "#     'CoapplicantIncome', \n",
    "#     'LoanAmount', \n",
    "#     'Total_Income', \n",
    "#     'Loan_Amount_to_Loan_Term_Ratio', \n",
    "#     'CreditHistory_TotalIncome', \n",
    "#     'Income_Per_LoanTerm',\n",
    "#     'ApplicantIncome_Squared',               # Newly added squared feature\n",
    "#     'LoanAmount_TotalIncome_Interaction',    # Newly added interaction term\n",
    "# ] + list(poly_feature_names)                # Add polynomial feature names dynamically\n",
    "\n",
    "# # Scale continuous features\n",
    "# scaler = StandardScaler()\n",
    "# train[continuous_features] = scaler.fit_transform(train[continuous_features])\n",
    "# test[continuous_features] = scaler.transform(test[continuous_features])\n",
    "\n",
    "# # Save processed datasets\n",
    "# train.to_csv('data/Train_Processed.csv', index=True)\n",
    "# test.to_csv('data/Test_Processed.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Reload Original Dataset\n",
    "train = pd.read_csv('data/Train.csv')\n",
    "hidden_set = pd.read_csv('data/Test.csv')  # Rename test set to hidden_set\n",
    "\n",
    "# Step 2: Rename and set ID columns\n",
    "train.set_index('ID', inplace=True)\n",
    "hidden_set.set_index('ID', inplace=True)\n",
    "\n",
    "# Step 3: Preprocess Dataset\n",
    "# Drop Loan_ID column\n",
    "train.drop('Loan_ID', axis=1, inplace=True)\n",
    "hidden_set.drop('Loan_ID', axis=1, inplace=True)\n",
    "\n",
    "# Handle Dependents column\n",
    "train['Dependents'] = train['Dependents'].replace('3+', 3).astype(int)\n",
    "hidden_set['Dependents'] = hidden_set['Dependents'].replace('3+', 3).astype(int)\n",
    "train['Dependents_Grouped'] = train['Dependents'].apply(lambda x: 2 if x >= 2 else x)\n",
    "hidden_set['Dependents_Grouped'] = hidden_set['Dependents'].apply(lambda x: 2 if x >= 2 else x)\n",
    "\n",
    "# Calculate Total_Income\n",
    "train['Total_Income'] = train['ApplicantIncome'] + train['CoapplicantIncome']\n",
    "hidden_set['Total_Income'] = hidden_set['ApplicantIncome'] + hidden_set['CoapplicantIncome']\n",
    "\n",
    "# Apply log transformation\n",
    "log_features = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Total_Income']\n",
    "for feature in log_features:\n",
    "    train[f'Log_{feature}'] = np.log1p(train[feature])\n",
    "    hidden_set[f'Log_{feature}'] = np.log1p(hidden_set[feature])\n",
    "\n",
    "# Add interaction features\n",
    "train['Loan_Amount_to_Loan_Term_Ratio'] = train['LoanAmount'] / train['Loan_Amount_Term']\n",
    "hidden_set['Loan_Amount_to_Loan_Term_Ratio'] = hidden_set['LoanAmount'] / hidden_set['Loan_Amount_Term']\n",
    "\n",
    "train['CreditHistory_TotalIncome'] = train['Credit_History'] * train['Total_Income']\n",
    "hidden_set['CreditHistory_TotalIncome'] = hidden_set['Credit_History'] * hidden_set['Total_Income']\n",
    "\n",
    "train['Income_Per_LoanTerm'] = train['Total_Income'] / train['Loan_Amount_Term']\n",
    "hidden_set['Income_Per_LoanTerm'] = hidden_set['Total_Income'] / hidden_set['Loan_Amount_Term']\n",
    "\n",
    "# Add missing interaction features\n",
    "train['ApplicantIncome_Squared'] = train['ApplicantIncome'] ** 2\n",
    "hidden_set['ApplicantIncome_Squared'] = hidden_set['ApplicantIncome'] ** 2\n",
    "\n",
    "train['LoanAmount_TotalIncome_Interaction'] = train['LoanAmount'] * train['Total_Income']\n",
    "hidden_set['LoanAmount_TotalIncome_Interaction'] = hidden_set['LoanAmount'] * hidden_set['Total_Income']\n",
    "\n",
    "train[\"Income_Credit_Interaction\"] = train[\"ApplicantIncome\"] * train[\"CreditHistory_TotalIncome\"]\n",
    "hidden_set[\"Income_Credit_Interaction\"] = hidden_set[\"ApplicantIncome\"] * hidden_set[\"CreditHistory_TotalIncome\"]\n",
    "\n",
    "# Square Root Transformation for LoanAmount\n",
    "train[\"LoanAmount_Sqrt\"] = np.sqrt(train[\"LoanAmount\"])\n",
    "hidden_set[\"LoanAmount_Sqrt\"] = np.sqrt(hidden_set[\"LoanAmount\"])\n",
    "\n",
    "# Ratio of Dependents to Total Income\n",
    "train[\"Dependents_Income_Ratio\"] = train[\"Dependents\"] / (train[\"Total_Income\"] + 1e-6)\n",
    "hidden_set[\"Dependents_Income_Ratio\"] = hidden_set[\"Dependents\"] / (hidden_set[\"Total_Income\"] + 1e-6)\n",
    "\n",
    "# # Polynomial features\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "train_poly_features = poly.fit_transform(train[['ApplicantIncome', 'LoanAmount']])\n",
    "hidden_set_poly_features = poly.transform(hidden_set[['ApplicantIncome', 'LoanAmount']])\n",
    "\n",
    "poly_feature_names = poly.get_feature_names_out(['ApplicantIncome', 'LoanAmount'])\n",
    "train[poly_feature_names] = train_poly_features\n",
    "hidden_set[poly_feature_names] = hidden_set_poly_features\n",
    "\n",
    "\n",
    "# Binning Loan_Amount_Term\n",
    "bins = [0, 180, 360, float('inf')]\n",
    "labels = ['Short-term', 'Medium-term', 'Long-term']\n",
    "train['Loan_Term_Bin'] = pd.cut(train['Loan_Amount_Term'], bins=bins, labels=labels)\n",
    "hidden_set['Loan_Term_Bin'] = pd.cut(hidden_set['Loan_Amount_Term'], bins=bins, labels=labels)\n",
    "\n",
    "# One-hot encode categorical features\n",
    "categorical_features = ['Education', 'Self_Employed', 'Property_Area', 'Dependents_Grouped', 'Loan_Term_Bin']\n",
    "train = pd.get_dummies(train, columns=categorical_features, drop_first=True).astype(int)\n",
    "hidden_set = pd.get_dummies(hidden_set, columns=categorical_features, drop_first=True).astype(int)\n",
    "\n",
    "\n",
    "# Outlier treatment\n",
    "cap_thresholds = {\n",
    "    'ApplicantIncome': train['ApplicantIncome'].quantile(0.99),\n",
    "    'LoanAmount': train['LoanAmount'].quantile(0.99),\n",
    "}\n",
    "for feature, cap in cap_thresholds.items():\n",
    "    train[feature] = np.clip(train[feature], None, cap)\n",
    "    hidden_set[feature] = np.clip(hidden_set[feature], None, cap)\n",
    "\n",
    "# Continuous features\n",
    "continuous_features = [\n",
    "    'ApplicantIncome', \n",
    "    'CoapplicantIncome', \n",
    "    'LoanAmount', \n",
    "    'Loan_Amount_Term', \n",
    "    'Total_Income', \n",
    "    'Log_ApplicantIncome', \n",
    "    'Log_CoapplicantIncome', \n",
    "    'Log_LoanAmount', \n",
    "    'Log_Total_Income', \n",
    "    'Loan_Amount_to_Loan_Term_Ratio', \n",
    "    'CreditHistory_TotalIncome', \n",
    "    'Income_Per_LoanTerm', \n",
    "    'ApplicantIncome_Squared', \n",
    "    'LoanAmount_TotalIncome_Interaction', \n",
    "    'Income_Credit_Interaction', \n",
    "    'LoanAmount_Sqrt', \n",
    "    'Dependents_Income_Ratio', \n",
    "    'ApplicantIncome', \n",
    "    'ApplicantIncome LoanAmount', \n",
    "    'LoanAmount^2'\n",
    "]\n",
    "\n",
    "# Scale continuous features\n",
    "scaler = StandardScaler()\n",
    "train[continuous_features] = scaler.fit_transform(train[continuous_features])\n",
    "hidden_set[continuous_features] = scaler.transform(hidden_set[continuous_features])\n",
    "\n",
    "# Step 4: Split train into train and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate features and target\n",
    "X = train.drop('Loan_Status', axis=1)\n",
    "y = train['Loan_Status']\n",
    "\n",
    "# Step 1: Split into train_data (70%) and temp_data (30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# resampler = SMOTE(random_state=42)\n",
    "# X_train, y_train = resampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# Display sizes of each split\n",
    "print(f\"Train set size: {X_train.shape}, {y_train.shape}\")\n",
    "# print(f\"Validation set size: {X_val.shape}, {y_val.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}, {y_test.shape}\")\n",
    "print(f\"Hidden set size: {hidden_set.shape}\")\n",
    "\n",
    "# # Step 3: Save processed datasets\n",
    "# train_data = X_train.copy()\n",
    "# train_data['Loan_Status'] = y_train\n",
    "# train_data.to_csv('data/Train_Split.csv', index=True)\n",
    "\n",
    "# val_data = X_val.copy()\n",
    "# val_data['Loan_Status'] = y_val\n",
    "# val_data.to_csv('data/Val_Split.csv', index=True)\n",
    "\n",
    "# hidden_set.to_csv('data/Hidden_Set_Processed.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe(include='all').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_list = []\n",
    "numerical_list = []\n",
    "\n",
    "for i in train.columns:\n",
    "    print(f\"{i}: {train[i].nunique()}\")\n",
    "    if train[i].nunique() <=5:\n",
    "        encode_list.append(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split_name, y_split in zip(['Train', 'Test'], [y_train, y_test]):\n",
    "    print(f\"{split_name} class distribution:\\n{y_split.value_counts(normalize=True)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_1 = {'n_estimators': 992,\n",
    " 'learning_rate': 0.024273593878709077,\n",
    " 'num_leaves': 103,\n",
    " 'max_depth': 12,\n",
    " 'min_child_samples': 37,\n",
    " 'feature_fraction': 0.6137915003281622,\n",
    " 'bagging_fraction': 0.6072555152182413,\n",
    " 'bagging_freq': 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from lightgbm import early_stopping\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "def objective(trial, X_train, y_train):\n",
    "    # Define hyperparameters\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_error',  # Accuracy-focused metric\n",
    "        'boosting_type': 'gbdt',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.5, 1.0),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.5, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "    }\n",
    "\n",
    "    # Initialize SMOTE\n",
    "    smote = SMOTE(random_state=42)\n",
    "\n",
    "    # Stratified Cross-Validation\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = []\n",
    "    for train_idx, val_idx in skf.split(X_train, y_train):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        # Apply SMOTE to the training data\n",
    "        X_tr_smote, y_tr_smote = smote.fit_resample(X_tr, y_tr)\n",
    "\n",
    "        # Train LightGBM model\n",
    "        model = LGBMClassifier(**params, random_state=42)\n",
    "        model.fit(\n",
    "            X_tr_smote, \n",
    "            y_tr_smote, \n",
    "            eval_set=[(X_val, y_val)], \n",
    "            eval_metric='binary_error', \n",
    "            callbacks=[early_stopping(stopping_rounds=50, verbose=False)]\n",
    "        )\n",
    "\n",
    "        # Validate the model\n",
    "        val_preds = model.predict(X_val)\n",
    "        cv_scores.append(accuracy_score(y_val, val_preds))\n",
    "\n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "# Prepare your train data: X_train and y_train\n",
    "X_train = train.drop('Loan_Status', axis=1)\n",
    "y_train = train['Loan_Status']\n",
    "\n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(lambda trial: objective(trial, X_train, y_train), n_trials=200)\n",
    "\n",
    "# Best parameters and accuracy\n",
    "print(\"Best Parameters:\", study.best_params)\n",
    "print(\"Best Accuracy:\", study.best_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'n_estimators': 547,\n",
    " 'learning_rate': 0.08066562616278325,\n",
    " 'num_leaves': 103,\n",
    " 'max_depth': 15,\n",
    " 'min_child_samples': 18,\n",
    " 'feature_fraction': 0.5453253046633786,\n",
    " 'bagging_fraction': 0.9107055079871386,\n",
    " 'bagging_freq': 8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the classifier with class_weight\n",
    "final_model = LGBMClassifier(**study.best_params, class_weight='balanced', random_state=42)\n",
    "final_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_test_pred = final_model.predict(X_test)\n",
    "\n",
    "# Confusion matrix and classification report\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# Evaluate predictions\n",
    "print(\"Test Set Results:\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Get predicted probabilities\n",
    "y_probs = final_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute precision-recall pairs\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, y_probs, pos_label=1)\n",
    "\n",
    "# Compute F1 scores\n",
    "f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-6)\n",
    "\n",
    "# Find the threshold with the highest F1 score\n",
    "optimal_idx = np.argmax(f1_scores)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "print(\"Optimal Threshold:\", optimal_threshold)\n",
    "\n",
    "# Make predictions based on the optimal threshold\n",
    "y_val_pred = (y_probs >= optimal_threshold).astype(int)\n",
    "print(\"\\nAccuracy:\", accuracy_score(y_test, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Confusion Matrix with Optimal Threshold:\")\n",
    "print(confusion_matrix(y_test, y_val_pred))\n",
    "\n",
    "print(\"\\nClassification Report with Optimal Threshold:\")\n",
    "print(classification_report(y_test, y_val_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the hidden set\n",
    "hidden_predictions = final_model.predict(hidden_set)\n",
    "\n",
    "# Since the hidden set does not have true labels, output predictions for submission\n",
    "hidden_set['Loan_Status_Prediction'] = hidden_predictions\n",
    "\n",
    "# Save predictions for submission or further analysis\n",
    "hidden_set[['Loan_Status_Prediction']].to_csv('submssions/lgbm_hse_006.csv', index=True)\n",
    "\n",
    "print(\"Predictions for the hidden set saved to 'Hidden_Set_Predictions.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reanalysing Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Select key features to compare\n",
    "key_features = ['ApplicantIncome', 'LoanAmount', 'Total_Income', 'Credit_History']\n",
    "\n",
    "# Plot feature distributions\n",
    "for feature in key_features:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.kdeplot(train[feature], label='Train', fill=True, alpha=0.5)\n",
    "    sns.kdeplot(X_val[feature], label='Validation', fill=True, alpha=0.5)\n",
    "    sns.kdeplot(X_test[feature], label='Test', fill=True, alpha=0.5)\n",
    "    sns.kdeplot(hidden_set[feature], label='Hidden', fill=True, alpha=0.5)\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis of Distributions\n",
    "The density plots reveal that some features (e.g., ApplicantIncome, Total_Income, Credit_History) exhibit discrepancies between the hidden test set and the other sets. These differences could explain the poor performance on the hidden test set.\n",
    "\n",
    "Action Plan:\n",
    "\n",
    "* Quantify these differences using statistical tests (e.g., KS-test, Wasserstein distance) to measure the divergence between distributions.\n",
    "* Investigate why the hidden test set differs. This could be due to data leakage, preprocessing differences, or shifts in feature distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted probabilities\n",
    "hidden_preds_proba = final_model.predict_proba(hidden_set.drop('Loan_Status_Prediction', axis=1))[:, 1]\n",
    "\n",
    "# Check the confidence distribution of predictions\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(hidden_preds_proba, kde=True, bins=30)\n",
    "plt.title('Prediction Confidence on Hidden Set')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Calibration\n",
    "The calibration curve shows the model is overconfident for certain probability ranges. Miscalibration can lead to suboptimal decisions based on predicted probabilities.\n",
    "\n",
    "Action Plan:\n",
    "\n",
    "* Recalibrate the model using techniques like Platt Scaling or Isotonic Regression on the validation set.\n",
    "* Validate calibration improvements by re-evaluating the model's probability predictions on the hidden test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = final_model.feature_importances_\n",
    "features = X_test.columns\n",
    "importance_df = pd.DataFrame({'Feature': features, 'Importance': importances}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=importance_df.head(20))\n",
    "plt.title('Top 20 Feature Importances')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Importance Insights\n",
    "The feature importance plot highlights the top contributors (e.g., LoanAmount_TotalIncome_Interaction, Loan_Amount_to_Loan_Term_Ratio).\n",
    "\n",
    "Action Plan:\n",
    "\n",
    "* Check the stability of these features across train/validation/test/hidden sets.\n",
    "* Test whether removing or transforming top features improves generalization. For instance, some features may have noise or overfitting tendencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Get predicted probabilities and true labels for validation set\n",
    "y_val_pred_proba = final_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Calibration curve\n",
    "prob_true, prob_pred = calibration_curve(y_val, y_val_pred_proba, n_bins=10)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(prob_pred, prob_true, marker='o', label='Model Calibration')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', label='Perfect Calibration')\n",
    "plt.xlabel('Mean Predicted Probability')\n",
    "plt.ylabel('Fraction of Positives')\n",
    "plt.title('Calibration Curve')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regularization and Complexity Control\n",
    "Overfitting may arise from a lack of regularization or overly complex models.\n",
    "\n",
    "Action Plan:\n",
    "\n",
    "* Tune hyperparameters like num_leaves, max_depth, and min_data_in_leaf to reduce model complexity.\n",
    "* Use techniques like dropout, shuffling, or cross-validation to better generalize the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Address Hidden Test Set Challenges\n",
    "Since we don't have true labels for the hidden set, here's how we would proceed:\n",
    "\n",
    "* Use pseudo-labeling: Predict on the hidden test set, assign high-confidence predictions as pseudo-labels, and fine-tune the model.\n",
    "* Split the hidden test set into subsets. For example, use a small part for manual validation if possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reattempting Upgrading the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Evaluate Feature Distributions Across Sets\n",
    "We'll check if there are major distributional differences between the training, validation, test, and hidden sets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List of continuous numeric features to analyze\n",
    "numeric_features = [\n",
    "    'ApplicantIncome', \n",
    "    'CoapplicantIncome', \n",
    "    'LoanAmount', \n",
    "    'Total_Income', \n",
    "    'Loan_Amount_to_Loan_Term_Ratio', \n",
    "    'CreditHistory_TotalIncome', \n",
    "    'Income_Per_LoanTerm',\n",
    "    'ApplicantIncome_Squared', \n",
    "    'LoanAmount_TotalIncome_Interaction'\n",
    "] + list(poly_feature_names)\n",
    "\n",
    "# Combine all datasets for comparison\n",
    "X_train['Dataset'] = 'Train'\n",
    "X_val['Dataset'] = 'Validation'\n",
    "X_test['Dataset'] = 'Test'\n",
    "hidden_set['Dataset'] = 'Hidden'\n",
    "\n",
    "combined_data = pd.concat([X_train, X_val, X_test, hidden_set])\n",
    "\n",
    "# Plot distributions for each feature\n",
    "for feature in numeric_features:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.kdeplot(data=combined_data, x=feature, hue='Dataset', fill=True)\n",
    "    plt.title(f'Distribution of {feature} Across Datasets')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend(title='Dataset')\n",
    "    plt.show()\n",
    "\n",
    "# Remove the 'Dataset' column after analysis\n",
    "X_train.drop('Dataset', axis=1, inplace=True)\n",
    "X_val.drop('Dataset', axis=1, inplace=True)\n",
    "X_test.drop('Dataset', axis=1, inplace=True)\n",
    "hidden_set.drop('Dataset', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distributional Issues:\n",
    "\n",
    "From the density plots, certain features like LoanAmount, ApplicantIncome_Squared, and interaction terms show skewed distributions across datasets. This could be contributing to poor generalization, as the hidden set may not align with training distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Predict probabilities on validation set\n",
    "y_val_prob = final_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Generate calibration curve\n",
    "prob_true, prob_pred = calibration_curve(y_val, y_val_prob, n_bins=10, strategy='uniform')\n",
    "\n",
    "# Plot the calibration curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(prob_pred, prob_true, marker='o', label='Model Calibration')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', label='Perfect Calibration')\n",
    "plt.title('Calibration Curve (Validation Set)')\n",
    "plt.xlabel('Mean Predicted Probability')\n",
    "plt.ylabel('Fraction of Positives')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Insights from Calibration Curve:\n",
    "\n",
    "The model is overconfident in its predictions, as seen from the sharp deviations from the perfect calibration line. This indicates potential overfitting or issues in probability calibration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities on hidden set\n",
    "y_hidden_prob = final_model.predict_proba(hidden_set.drop('Loan_Status_Prediction', axis=1))[:, 1]\n",
    "\n",
    "# Plot distribution of predicted probabilities\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(y_hidden_prob, bins=20, kde=True, color='blue')\n",
    "plt.title('Prediction Confidence on Hidden Set')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Low Confidence Predictions:\n",
    "\n",
    "There are significant low-confidence predictions (185 cases). These might stem from the overlap or lack of discrimination in certain features, suggesting areas for further feature engineering or model refinement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = final_model.feature_importances_\n",
    "features = X_train.columns\n",
    "\n",
    "# Sort feature importances in descending order\n",
    "sorted_indices = importances.argsort()[::-1]\n",
    "sorted_features = features[sorted_indices]\n",
    "sorted_importances = importances[sorted_indices]\n",
    "\n",
    "# Plot top 20 feature importances\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(sorted_features[:20][::-1], sorted_importances[:20][::-1])\n",
    "plt.title('Top 20 Feature Importances')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Importance:\n",
    "\n",
    "The top features like LoanAmount_TotalIncome_Interaction and Loan_Amount_to_Loan_Term_Ratio are highly influential, but some mid-ranked features might require further exploration for their interaction effects or transformation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predictions to hidden set\n",
    "hidden_set['Predicted_Probability'] = y_hidden_prob\n",
    "\n",
    "# Flag outliers based on prediction confidence (e.g., low confidence)\n",
    "low_confidence_threshold = 0.6  # Example threshold\n",
    "outliers = hidden_set[hidden_set['Predicted_Probability'] < low_confidence_threshold]\n",
    "\n",
    "# Display the flagged outliers\n",
    "print(\"Number of low-confidence predictions:\", len(outliers))\n",
    "print(outliers.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Dive into Low Confidence Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Steps for Low-Confidence Analysis\n",
    "1. Basic Statistics for Low-Confidence Cases:\n",
    "Compare the feature means, medians, and ranges of low-confidence samples to the full dataset.\n",
    "2. Feature Distribution Comparison:\n",
    "Plot the feature distributions for low-confidence predictions versus the full dataset. Identify features where the distributions diverge significantly.\n",
    "3. Correlation Analysis:\n",
    "Calculate correlations between features for low-confidence samples and compare them to the correlations in the full dataset.\n",
    "4. Cluster Analysis:\n",
    "Use clustering (e.g., K-Means or hierarchical clustering) on low-confidence samples to see if specific patterns or groups emerge.\n",
    "5. Feature Importance for Low-Confidence Predictions:\n",
    "Refit a model using only low-confidence samples and examine the feature importance. This helps identify what features drive uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predict labels (0 or 1) for the hidden set\n",
    "# y_pred_hidden = final_model.predict(hidden_set)\n",
    "\n",
    "# # Predict probabilities for the hidden set (for the positive class, 1)\n",
    "# y_pred_prob_hidden = final_model.predict_proba(hidden_set)[:, 1]  # Get probabilities for class 1\n",
    "\n",
    "# Add these predictions to the hidden_set\n",
    "y_pred_hidden = hidden_set['Loan_Status_Prediction']\n",
    "y_pred_prob_hidden = hidden_set['Predicted_Probability']\n",
    "\n",
    "# Display the updated hidden_set with predictions\n",
    "print(\"Updated Hidden Set with Predictions:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_predictions = hidden_set.copy()\n",
    "full_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Filter low-confidence samples\n",
    "low_confidence = full_predictions[full_predictions['Predicted_Probability'].between(0.4, 0.6)]\n",
    "\n",
    "# 1. Basic Statistics\n",
    "print(\"Basic Statistics for Low-Confidence Predictions:\")\n",
    "print(low_confidence.describe())\n",
    "\n",
    "print(\"\\nComparing Means with Full Dataset:\")\n",
    "print(\"Full Dataset Means:\\n\", train[continuous_features].mean())\n",
    "print(\"Low Confidence Means:\\n\", low_confidence[continuous_features].mean())\n",
    "\n",
    "# 2. Feature Distribution Comparison\n",
    "for feature in continuous_features:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.kdeplot(data=train, x=feature, label='Full Dataset', fill=True, alpha=0.5)\n",
    "    sns.kdeplot(data=low_confidence, x=feature, label='Low Confidence', fill=True, alpha=0.5, color='red')\n",
    "    plt.title(f\"Feature Distribution: {feature}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# 3. Correlation Analysis\n",
    "print(\"\\nCorrelation Analysis for Low-Confidence Predictions:\")\n",
    "low_conf_corr = low_confidence[continuous_features].corr()\n",
    "full_data_corr = train[continuous_features].corr()\n",
    "\n",
    "# Heatmaps for correlations\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(low_conf_corr, annot=True, cmap='coolwarm')\n",
    "plt.title(\"Low-Confidence Correlation Matrix\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(full_data_corr, annot=True, cmap='coolwarm')\n",
    "plt.title(\"Full Dataset Correlation Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# 4. Clustering Analysis\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "low_confidence['Cluster'] = kmeans.fit_predict(low_confidence[continuous_features])\n",
    "\n",
    "print(\"\\nCluster Distribution for Low-Confidence Predictions:\")\n",
    "print(low_confidence['Cluster'].value_counts())\n",
    "\n",
    "# Visualize clusters (using two key features)\n",
    "sns.scatterplot(\n",
    "    data=low_confidence, x=\"LoanAmount\", y=\"Total_Income\", hue=\"Cluster\", palette=\"viridis\"\n",
    ")\n",
    "plt.title(\"Clustering Low-Confidence Predictions\")\n",
    "plt.show()\n",
    "\n",
    "# 5. Feature Importance for Low-Confidence Cases\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Remove duplicate features from continuous_features\n",
    "continuous_features = list(dict.fromkeys(continuous_features))\n",
    "\n",
    "# Prepare data\n",
    "X_low_conf = low_confidence[continuous_features].copy()\n",
    "y_low_conf = low_confidence['Loan_Status_Prediction']\n",
    "\n",
    "# Ensure no duplicate columns in X_low_conf\n",
    "X_low_conf = X_low_conf.loc[:, ~X_low_conf.columns.duplicated()]\n",
    "\n",
    "# Verify that all columns are unique\n",
    "assert X_low_conf.columns.duplicated().sum() == 0, \"There are still duplicate columns in X_low_conf\"\n",
    "\n",
    "# Fit a LightGBM model\n",
    "low_conf_model = LGBMClassifier(random_state=42)\n",
    "low_conf_model.fit(X_low_conf, y_low_conf)\n",
    "\n",
    "# Plot feature importances\n",
    "low_conf_feature_importances = pd.DataFrame({\n",
    "    'Feature': X_low_conf.columns,\n",
    "    'Importance': low_conf_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display top feature importances\n",
    "print(low_conf_feature_importances)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=low_conf_feature_importances, x='Importance', y='Feature')\n",
    "plt.title(\"Feature Importances for Low-Confidence Predictions\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observations:\n",
    "\n",
    "1. Low-Confidence Predictions:\n",
    "\n",
    "    * These predictions are identified with probabilities between 0.4 and 0.6, representing ambiguous classifications.\n",
    "    * The total count of low-confidence samples is 106, and these were analyzed separately for feature distributions, clustering, and feature importance.\n",
    "\n",
    "2. Feature Importance:\n",
    "    * Key features contributing to the prediction model include:\n",
    "        * Loan_Amount_to_Loan_Term_Ratio\n",
    "        * CoapplicantIncome\n",
    "        * ApplicantIncome\n",
    "        * LoanAmount_TotalIncome_Interaction\n",
    "    * Features like LoanAmount^2 and ApplicantIncome^2 have minimal importance, suggesting possible redundancy.\n",
    "\n",
    "3. Feature Distributions:\n",
    "    * Comparing distributions for low-confidence predictions and the full dataset reveals noticeable differences in features such as ApplicantIncome, LoanAmount, and Total_Income.\n",
    "    * This indicates that these features are likely influencing uncertainty in the model's decision-making.\n",
    "\n",
    "4. Correlation Analysis:\n",
    "    * The correlation matrix for low-confidence predictions shows weaker relationships between some features compared to the full dataset, which could explain inconsistencies in predictions.\n",
    "    \n",
    "5. Cluster Analysis:\n",
    "    * Clustering low-confidence predictions using KMeans resulted in three distinct clusters:\n",
    "        * Cluster 0: Majority of the samples\n",
    "        * Cluster 2: Significant minority\n",
    "        * Cluster 1: Outlier cluster with only one sample\n",
    "    * This clustering provides insights into the variation within low-confidence predictions and can inform targeted re-training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scatter plot: Loan_Amount_to_Loan_Term_Ratio vs LoanAmount_TotalIncome_Interaction\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(\n",
    "    data=low_confidence,\n",
    "    x='Loan_Amount_to_Loan_Term_Ratio',\n",
    "    y='LoanAmount_TotalIncome_Interaction',\n",
    "    hue='Loan_Status_Prediction',  # Colored by the prediction\n",
    "    palette='viridis',\n",
    "    alpha=0.8\n",
    ")\n",
    "plt.title(\"Interaction: Loan_Amount_to_Loan_Term_Ratio vs LoanAmount_TotalIncome_Interaction\")\n",
    "plt.xlabel(\"Loan_Amount_to_Loan_Term_Ratio\")\n",
    "plt.ylabel(\"LoanAmount_TotalIncome_Interaction\")\n",
    "plt.legend(title=\"Loan Status\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of correlations for important features\n",
    "important_features = [\n",
    "    'Loan_Amount_to_Loan_Term_Ratio',\n",
    "    'LoanAmount_TotalIncome_Interaction',\n",
    "    'CreditHistory_TotalIncome',\n",
    "    'Total_Income'\n",
    "]\n",
    "\n",
    "correlation_matrix = low_confidence[important_features].corr()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    correlation_matrix,\n",
    "    annot=True,\n",
    "    cmap='coolwarm',\n",
    "    fmt='.2f'\n",
    ")\n",
    "plt.title(\"Correlation Heatmap of Important Features\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot for important interactions\n",
    "sns.pairplot(\n",
    "    data=low_confidence,\n",
    "    vars=['Loan_Amount_to_Loan_Term_Ratio', 'LoanAmount_TotalIncome_Interaction', 'Total_Income'],\n",
    "    hue='Loan_Status_Prediction',\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.suptitle(\"Pairplot for Important Features (Colored by Loan Status)\", y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Features to test\n",
    "features_to_test = [\n",
    "    'Loan_Amount_to_Loan_Term_Ratio',\n",
    "    'LoanAmount_TotalIncome_Interaction',\n",
    "    'Total_Income',\n",
    "    'CreditHistory_TotalIncome'\n",
    "]\n",
    "\n",
    "# Perform t-tests for each feature\n",
    "t_test_results = []\n",
    "for feature in features_to_test:\n",
    "    group_0 = low_confidence[low_confidence['Loan_Status_Prediction'] == 0][feature]\n",
    "    group_1 = low_confidence[low_confidence['Loan_Status_Prediction'] == 1][feature]\n",
    "    \n",
    "    # Perform t-test\n",
    "    t_stat, p_value = ttest_ind(group_0, group_1, equal_var=False)  # Assuming unequal variance\n",
    "    t_test_results.append({'Feature': feature, 't-statistic': t_stat, 'p-value': p_value})\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "import pandas as pd\n",
    "t_test_results_df = pd.DataFrame(t_test_results)\n",
    "\n",
    "# # Display results\n",
    "# import ace_tools as tools; tools.display_dataframe_to_user(name=\"T-Test Results for Features\", dataframe=t_test_results_df)\n",
    "# co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_test_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 'CreditHistory_TotalIncome',  # Removed\n",
    "    # 'Income_Per_LoanTerm',        # Removed\n",
    "    # 'ApplicantIncome_Squared',    # Removed\n",
    "    # 'LoanAmount_TotalIncome_Interaction',  # Removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify Features by Dropping Low Importance and high VIF Features\n",
    "low_importance_features = [\n",
    "    \"Education_1\", \n",
    "    \"Log_LoanAmount\", \n",
    "    \"Married_1\",\n",
    "    # \"CreditHistory_TotalIncome\",\n",
    "    \"Income_Per_LoanTerm\",\n",
    "    \"ApplicantIncome_Squared\",\n",
    "    \"LoanAmount_TotalIncome_Interaction\"\n",
    "\n",
    "]\n",
    "X_train_simplified = X_train.drop(columns=low_importance_features, errors=\"ignore\")\n",
    "X_val_simplified = X_val.drop(columns=low_importance_features, errors=\"ignore\")\n",
    "X_test_simplified = X_test.drop(columns=low_importance_features, errors=\"ignore\")\n",
    "hidden_set_simplified = hidden_set.drop(columns=low_importance_features, errors=\"ignore\")\n",
    "\n",
    "# Add Enriched Features\n",
    "# Example 1: Interaction between ApplicantIncome and CreditHistory_TotalIncome\n",
    "X_train_simplified[\"Income_Credit_Interaction\"] = (\n",
    "    X_train_simplified[\"ApplicantIncome\"] * X_train_simplified[\"CreditHistory_TotalIncome\"]\n",
    ")\n",
    "X_val_simplified[\"Income_Credit_Interaction\"] = (\n",
    "    X_val_simplified[\"ApplicantIncome\"] * X_val_simplified[\"CreditHistory_TotalIncome\"]\n",
    ")\n",
    "X_test_simplified[\"Income_Credit_Interaction\"] = (\n",
    "    X_test_simplified[\"ApplicantIncome\"] * X_test_simplified[\"CreditHistory_TotalIncome\"]\n",
    ")\n",
    "hidden_set_simplified[\"Income_Credit_Interaction\"] = (\n",
    "    hidden_set_simplified[\"ApplicantIncome\"] * hidden_set_simplified[\"CreditHistory_TotalIncome\"]\n",
    ")\n",
    "\n",
    "# Example 2: Square Root Transformation for LoanAmount\n",
    "X_train_simplified[\"LoanAmount_Sqrt\"] = np.sqrt(X_train_simplified[\"LoanAmount\"])\n",
    "X_val_simplified[\"LoanAmount_Sqrt\"] = np.sqrt(X_val_simplified[\"LoanAmount\"])\n",
    "X_test_simplified[\"LoanAmount_Sqrt\"] = np.sqrt(X_test_simplified[\"LoanAmount\"])\n",
    "hidden_set_simplified[\"LoanAmount_Sqrt\"] = np.sqrt(hidden_set_simplified[\"LoanAmount\"])\n",
    "\n",
    "# Example 3: Ratio of Dependents to Total Income\n",
    "X_train_simplified[\"Dependents_Income_Ratio\"] = (\n",
    "    X_train_simplified[\"Dependents\"] / (X_train_simplified[\"Total_Income\"] + 1e-6)\n",
    ")\n",
    "X_val_simplified[\"Dependents_Income_Ratio\"] = (\n",
    "    X_val_simplified[\"Dependents\"] / (X_val_simplified[\"Total_Income\"] + 1e-6)\n",
    ")\n",
    "X_test_simplified[\"Dependents_Income_Ratio\"] = (\n",
    "    X_test_simplified[\"Dependents\"] / (X_test_simplified[\"Total_Income\"] + 1e-6)\n",
    ")\n",
    "hidden_set_simplified[\"Dependents_Income_Ratio\"] = (\n",
    "    hidden_set_simplified[\"Dependents\"] / (hidden_set_simplified[\"Total_Income\"] + 1e-6)\n",
    ")\n",
    "\n",
    "# Ensure Columns are Aligned Across All Sets\n",
    "X_val_simplified = X_val_simplified.reindex(columns=X_train_simplified.columns, fill_value=0)\n",
    "X_test_simplified = X_test_simplified.reindex(columns=X_train_simplified.columns, fill_value=0)\n",
    "hidden_set_simplified = hidden_set_simplified.reindex(columns=X_train_simplified.columns, fill_value=0)\n",
    "\n",
    "# Save the Simplified and Enriched Datasets\n",
    "X_train_simplified.to_csv(\"data/X_Train_Simplified.csv\", index=True)\n",
    "X_val_simplified.to_csv(\"data/X_Val_Simplified.csv\", index=True)\n",
    "X_test_simplified.to_csv(\"data/X_Test_Simplified.csv\", index=True)\n",
    "hidden_set_simplified.to_csv(\"data/Hidden_Set_Simplified.csv\", index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_error',  # Accuracy-focused metric\n",
    "        'boosting_type': 'gbdt',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.5, 1.0),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.5, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "    }\n",
    "\n",
    "    # Stratified Cross-Validation\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = []\n",
    "    for train_idx, val_idx in skf.split(train.drop('Loan_Status', axis=1), train['Loan_Status']):\n",
    "        X_train, X_val = train.iloc[train_idx].drop('Loan_Status', axis=1), train.iloc[val_idx].drop('Loan_Status', axis=1)\n",
    "        y_train, y_val = train.iloc[train_idx]['Loan_Status'], train.iloc[val_idx]['Loan_Status']\n",
    "\n",
    "        model = LGBMClassifier(**params, random_state=42)\n",
    "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='binary_error', callbacks=[early_stopping(stopping_rounds=50, verbose=-1)])\n",
    "\n",
    "        val_preds = model.predict(X_val)\n",
    "        cv_scores.append(accuracy_score(y_val, val_preds))\n",
    "\n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Best parameters and accuracy\n",
    "print(\"Best Parameters:\", study.best_params)\n",
    "print(\"Best Accuracy:\", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from lightgbm import early_stopping\n",
    "import numpy as np\n",
    "\n",
    "# Combine Train and Validation sets (excluding test/hidden sets)\n",
    "combined_train = X_train.copy()\n",
    "y_train = combined_train['Loan_Status']\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_error',  # Accuracy-focused metric\n",
    "        'boosting_type': 'gbdt',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.5, 1.0),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.5, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "    }\n",
    "\n",
    "    # Stratified Cross-Validation (excluding test and validation sets)\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = []\n",
    "    for train_idx, val_idx in skf.split(combined_train.drop('Loan_Status', axis=1), combined_train['Loan_Status']):\n",
    "        # Use only part of training data\n",
    "        X_fold_train, X_fold_val = combined_train.iloc[train_idx].drop('Loan_Status', axis=1), combined_train.iloc[val_idx].drop('Loan_Status', axis=1)\n",
    "        y_fold_train, y_fold_val = combined_train.iloc[train_idx]['Loan_Status'], combined_train.iloc[val_idx]['Loan_Status']\n",
    "\n",
    "        model = LGBMClassifier(**params, random_state=42)\n",
    "        model.fit(\n",
    "            X_fold_train, \n",
    "            y_fold_train, \n",
    "            eval_set=[(X_fold_val, y_fold_val)], \n",
    "            eval_metric='binary_error', \n",
    "            callbacks=[early_stopping(stopping_rounds=50, verbose=-1)]\n",
    "        )\n",
    "\n",
    "        val_preds = model.predict(X_fold_val)\n",
    "        cv_scores.append(accuracy_score(y_fold_val, val_preds))\n",
    "\n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Best parameters and accuracy\n",
    "print(\"Best Parameters:\", study.best_params)\n",
    "print(\"Best Accuracy:\", study.best_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_simplified[\"Income_Credit_Interaction\"] = (\n",
    "    X_train_simplified[\"ApplicantIncome\"] * X_train_simplified[\"CreditHistory_TotalIncome\"]\n",
    ")\n",
    "\n",
    "# Example 2: Square Root Transformation for LoanAmount\n",
    "X_train_simplified[\"LoanAmount_Sqrt\"] = np.sqrt(X_train_simplified[\"LoanAmount\"])\n",
    "\n",
    "\n",
    "# Example 3: Ratio of Dependents to Total Income\n",
    "X_train_simplified[\"Dependents_Income_Ratio\"] = (\n",
    "    X_train_simplified[\"Dependents\"] / (X_train_simplified[\"Total_Income\"] + 1e-6)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Key Takeaways\n",
    "High Precision and Recall for Class 1 (Loan Approved):\n",
    "\n",
    "Precision: 94%\n",
    "Recall: 100%\n",
    "This indicates that the model is excellent at correctly identifying approved loans.\n",
    "Lower Performance for Class 0 (Loan Not Approved):\n",
    "\n",
    "Precision: 100%\n",
    "Recall: 67%\n",
    "While the model is very precise when predicting \"not approved,\" it misses about 28% of the true \"not approved\" cases.\n",
    "Imbalanced Recall:\n",
    "\n",
    "Recall imbalance suggests the model might still be skewed towards the majority class (approved loans). This is common with imbalanced datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Misclassification Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps for Misclassification Analysis\n",
    "1. Extract Misclassified Cases\n",
    "    * False Negatives (Class 0 predicted as Class 1): Instances where Loan_Status=0 but the model predicted Loan_Status=1.\n",
    "    * False Positives (Class 1 predicted as Class 0): Instances where Loan_Status=1 but the model predicted Loan_Status=0.\n",
    "\n",
    "2. Analyze Feature Distributions\n",
    "    * Compare the distributions of key features for correctly classified vs. misclassified instances:\n",
    "    * ApplicantIncome, CoapplicantIncome\n",
    "    * Credit_History\n",
    "    * LoanAmount, Total_Income\n",
    "    * Property_Area, Education, etc.\n",
    "\n",
    "3. Visualize Differences\n",
    "    * Use boxplots or KDE plots to compare distributions for features contributing to misclassification.\n",
    "    \n",
    "4. Identify Patterns: Check for\n",
    "    * Feature ranges where the model struggles (e.g., specific income or loan amounts).\n",
    "    * Feature interactions contributing to misclassification (e.g., high income but poor credit history).\n",
    "\n",
    "5. Propose Adjustments\n",
    "Based on insights, consider feature engineering, parameter tuning, or sampling strategies to address misclassified patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Get predictions and probabilities\n",
    "y_val_probs = final_model.predict_proba(X_val)[:, 1]\n",
    "y_val_pred = final_model.predict(X_val)\n",
    "\n",
    "# Extract misclassified cases\n",
    "X_val = X_val.reset_index(drop=True)\n",
    "y_val = y_val.reset_index(drop=True)\n",
    "\n",
    "# False Negatives (Actual = 0, Predicted = 1)\n",
    "fn_indices = (y_val == 0) & (y_val_pred == 1)\n",
    "false_negatives = X_val[fn_indices]\n",
    "\n",
    "# False Positives (Actual = 1, Predicted = 0)\n",
    "fp_indices = (y_val == 1) & (y_val_pred == 0)\n",
    "false_positives = X_val[fp_indices]\n",
    "\n",
    "# Correctly Classified\n",
    "correctly_classified = X_val[(y_val == y_val_pred)]\n",
    "\n",
    "# Summary\n",
    "print(f\"False Negatives: {false_negatives.shape[0]} cases\")\n",
    "print(f\"False Positives: {false_positives.shape[0]} cases\")\n",
    "print(f\"Correctly Classified: {correctly_classified.shape[0]} cases\")\n",
    "\n",
    "# Feature distribution comparison: ApplicantIncome example\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=pd.concat([\n",
    "    pd.DataFrame({'Feature': false_negatives['ApplicantIncome'], 'Type': 'False Negatives'}),\n",
    "    pd.DataFrame({'Feature': false_positives['ApplicantIncome'], 'Type': 'False Positives'}),\n",
    "    pd.DataFrame({'Feature': correctly_classified['ApplicantIncome'], 'Type': 'Correctly Classified'})\n",
    "]), x='Type', y='Feature')\n",
    "plt.title(\"ApplicantIncome Distribution Across Classification Types\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_low_conf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
