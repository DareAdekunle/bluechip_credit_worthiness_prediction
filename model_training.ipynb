{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# For data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For splitting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For resampling (handling class imbalance)\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "\n",
    "# For scaling numeric features\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# # Configurations (optional)\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train, test):\n",
    "    \"\"\"\n",
    "    Preprocesses train and test datasets with streamlined transformations and appropriate resampling.\n",
    "\n",
    "    Parameters:\n",
    "        train (DataFrame): Training dataset.\n",
    "        test (DataFrame): Testing dataset.\n",
    "\n",
    "    Returns:\n",
    "        train (DataFrame): Preprocessed training dataset.\n",
    "        test (DataFrame): Preprocessed testing dataset.\n",
    "    \"\"\"\n",
    "    # Step 1: Drop 'ID' and 'Loan_ID' as they are unique identifiers\n",
    "    train.drop(['ID', 'Loan_ID'], axis=1, inplace=True)\n",
    "    test.drop(['ID', 'Loan_ID'], axis=1, inplace=True)\n",
    "\n",
    "    # Step 2: Replace '3+' with 3 in 'Dependents' and convert to float\n",
    "    train['Dependents'] = train['Dependents'].replace('3+', 3).astype(float)\n",
    "    test['Dependents'] = test['Dependents'].replace('3+', 3).astype(float)\n",
    "\n",
    "    # Step 3: Handle missing values (if any)\n",
    "    # Although your dataset does not have missing values, this is good practice\n",
    "    numeric_cols = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term', 'Total_Income']\n",
    "    for col in numeric_cols:\n",
    "        train[col].fillna(train[col].median(), inplace=True)\n",
    "        test[col].fillna(test[col].median(), inplace=True)\n",
    "\n",
    "    categorical_cols = ['Credit_History', 'Self_Employed', 'Education', 'Gender', 'Married']\n",
    "    for col in categorical_cols:\n",
    "        train[col].fillna(train[col].mode()[0], inplace=True)\n",
    "        test[col].fillna(test[col].mode()[0], inplace=True)\n",
    "\n",
    "    # Ensure no NaN values remain in the dataset\n",
    "    train.fillna(0, inplace=True)\n",
    "    test.fillna(0, inplace=True)\n",
    "\n",
    "    # Step 4: One-Hot Encode 'Property_Area'\n",
    "    train = pd.get_dummies(train, columns=['Property_Area'], drop_first=True)\n",
    "    test = pd.get_dummies(test, columns=['Property_Area'], drop_first=True)\n",
    "\n",
    "    # Step 5: Add feature interactions with descriptive headers\n",
    "    for df in [train, test]:\n",
    "        # Basic interactions\n",
    "        df['Loan_to_Income_Ratio'] = df['LoanAmount'] / (df['ApplicantIncome'] + df['CoapplicantIncome'] + 1e-6)\n",
    "        df['Income_per_Dependent'] = df['Total_Income'] / (df['Dependents'] + 1)\n",
    "        df['LoanAmount_per_Term'] = df['LoanAmount'] / (df['Loan_Amount_Term'] + 1e-6)\n",
    "        df['EMI'] = df['LoanAmount'] / (df['Loan_Amount_Term'] + 1e-6)\n",
    "        df['EMI_to_Income_Ratio'] = df['EMI'] / df['Total_Income']\n",
    "        df['Debt_to_Income_Ratio'] = df['LoanAmount'] / (df['Total_Income'] + 1e-6)\n",
    "        df['all_income'] = df['ApplicantIncome'] + df['CoapplicantIncome'] + df['Total_Income']\n",
    "        df['loan_to_all_income_ratio'] = df['LoanAmount'] / df['all_income']\n",
    "        df['all_Income_per_Dependent'] = df['all_income'] / (df['Dependents'] + 1)\n",
    "\n",
    "        # Additional interactions\n",
    "        df['Dependents_Credit_History_Interaction'] = df['Dependents'] * df['Credit_History']\n",
    "        df['Income_LoanAmount_Interaction'] = df['Total_Income'] * df['LoanAmount']\n",
    "        df['all_Income_LoanAmount_Interaction'] = df['all_income'] * df['LoanAmount']\n",
    "        df['Credit_History_LoanAmount'] = df['Credit_History'] * df['LoanAmount']\n",
    "        df['Credit_History_Income_Interaction'] = df['Credit_History'] * df['Total_Income']\n",
    "        df['Credit_History_all_Income_Interaction'] = df['Credit_History'] * df['all_income']\n",
    "        df['loan_to_income_ratio'] = df['LoanAmount'] / (df['Total_Income'] + 1e-6)\n",
    "        df['income_per_year_emp'] = df['ApplicantIncome'] / (df['Loan_Amount_Term'] + 1e-6)\n",
    "        df['coapplicant_income_per_year_emp'] = df['CoapplicantIncome'] / (df['Loan_Amount_Term'] + 1e-6)\n",
    "        df['all_income_per_year_emp'] = df['all_income'] / (df['Loan_Amount_Term'] + 1e-6)\n",
    "        df['Coapplicant_Income_Ratio'] = df['CoapplicantIncome'] / df['Total_Income']\n",
    "        df['Applicant_Income_Ratio'] = df['ApplicantIncome'] / df['Total_Income']\n",
    "        df['TotalIncome_LoanAmount_Ratio'] = df['Total_Income'] / (df['LoanAmount'] + 1e-6)\n",
    "        df['all_Income_LoanAmount_Ratio'] = df['all_income'] / (df['LoanAmount'] + 1e-6)\n",
    "        df['CreditHistory_LoanTerm_Interaction'] = df['Credit_History'] * df['Loan_Amount_Term']\n",
    "        df['Education_LoanAmount_Interaction'] = df['Education'] * df['LoanAmount']\n",
    "        df['SelfEmployed_Income_Interaction'] = df['Self_Employed'] * df['Total_Income']\n",
    "\n",
    "        # Squared and log-transformed features\n",
    "        df['ApplicantIncome_Squared'] = df['ApplicantIncome'] ** 2\n",
    "        df['all_income_Squared'] = df['all_income'] ** 2\n",
    "        df['LoanAmount_Squared'] = df['LoanAmount'] ** 2\n",
    "        df['Log_LoanAmount_per_Term'] = np.log1p(df['LoanAmount_per_Term'])\n",
    "        df['Log_ApplicantIncome'] = np.log1p(df['ApplicantIncome'])\n",
    "        df['Log_CoapplicantIncome'] = np.log1p(df['CoapplicantIncome'])\n",
    "        df['Log_LoanAmount'] = np.log1p(df['LoanAmount'])\n",
    "        df['Log_Total_Income'] = np.log1p(df['Total_Income'])\n",
    "\n",
    "        # Dependents and loan-based interactions\n",
    "        df['Dependents_TotalIncome_Interaction'] = df['Dependents'] * df['Total_Income']\n",
    "        df['Dependents_LoanAmount_Interaction'] = df['Dependents'] * df['LoanAmount']\n",
    "        df['LoanAmount_to_Term_Ratio'] = df['LoanAmount'] / (df['Loan_Amount_Term'] + 1e-6)\n",
    "\n",
    "        # Feature interactions with Gender and Married\n",
    "        df['Gender_ApplicantIncome'] = df['Gender'] * df['ApplicantIncome']\n",
    "        df['Married_CoapplicantIncome'] = df['Married'] * df['CoapplicantIncome']\n",
    "        df['Married_Total_Income'] = df['Married'] * df['Total_Income']\n",
    "        df['Gender_Married_Interaction'] = df['Gender'] * df['Married']\n",
    "\n",
    "    return train, test\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "\n",
    "# Helper function for oversampling\n",
    "def resample_minority_class(X, y):\n",
    "    # Concatenate X and y for resampling\n",
    "    data = pd.concat([X, y], axis=1)\n",
    "    minority_class = data[y.name].value_counts().idxmin()  # Find the minority class\n",
    "    minority_data = data[data[y.name] == minority_class]\n",
    "    majority_data = data[data[y.name] != minority_class]\n",
    "\n",
    "    # Oversample the minority class\n",
    "    minority_upsampled = resample(minority_data,\n",
    "                                  replace=True,  # Sample with replacement\n",
    "                                  n_samples=len(majority_data),  # Match majority class size\n",
    "                                  random_state=42)\n",
    "\n",
    "    # Combine majority data with upsampled minority data\n",
    "    upsampled_data = pd.concat([majority_data, minority_upsampled])\n",
    "\n",
    "    return upsampled_data.drop(columns=[y.name]), upsampled_data[y.name]\n",
    "\n",
    "# Helper function for undersampling\n",
    "def resample_majority_class(X, y):\n",
    "    # Concatenate X and y for resampling\n",
    "    data = pd.concat([X, y], axis=1)\n",
    "    minority_class = data[y.name].value_counts().idxmin()  # Find the minority class\n",
    "    minority_data = data[data[y.name] == minority_class]\n",
    "    majority_data = data[data[y.name] != minority_class]\n",
    "\n",
    "    # Undersample the majority class\n",
    "    majority_downsampled = resample(majority_data,\n",
    "                                    replace=False,  # Sample without replacement\n",
    "                                    n_samples=len(minority_data),  # Match minority class size\n",
    "                                    random_state=42)\n",
    "\n",
    "    # Combine minority data with downsampled majority data\n",
    "    downsampled_data = pd.concat([minority_data, majority_downsampled])\n",
    "\n",
    "    return downsampled_data.drop(columns=[y.name]), downsampled_data[y.name]\n",
    "\n",
    "# Main function for resampling and scaling\n",
    "def resample_split(train, test, resampling_method='SMOTE', scaling_option=None):\n",
    "    \"\"\"\n",
    "    Splits the data into training and testing sets, handles class imbalance, and scales features appropriately.\n",
    "\n",
    "    Parameters:\n",
    "        train (DataFrame): Preprocessed training dataset.\n",
    "        test (DataFrame): Preprocessed testing dataset.\n",
    "        resampling_method (str): Resampling technique to handle class imbalance.\n",
    "                                 Options: 'SMOTE', 'ADASYN', 'Borderline-SMOTE', 'SMOTEENN', 'SMOTETomek', 'oversample', 'undersample'.\n",
    "        scaling_option (str): Scaling technique for continuous features.\n",
    "                              Options: 'standard', 'minmax', 'robust', or None.\n",
    "\n",
    "    Returns:\n",
    "        X_train_res (array): Resampled training features.\n",
    "        y_train_res (array): Resampled training labels.\n",
    "        X_test (array): Scaled testing features.\n",
    "        y_test (array): Original testing labels.\n",
    "        test_scaled (array): Scaled test dataset features.\n",
    "    \"\"\"\n",
    "    # Step 1: Create X and y\n",
    "    X = train.drop('Loan_Status', axis=1)\n",
    "    y = train['Loan_Status']\n",
    "    test = test.copy()\n",
    "\n",
    "    # Step 2: Align columns between train and test\n",
    "    X, test = X.align(test, join='left', axis=1, fill_value=0)\n",
    "\n",
    "    # Step 3: Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1, stratify=y)\n",
    "\n",
    "    # Step 4: Handle class imbalance\n",
    "    if resampling_method == 'SMOTE':\n",
    "        resampler = SMOTE(random_state=42)\n",
    "        X_train_res, y_train_res = resampler.fit_resample(X_train, y_train)\n",
    "    elif resampling_method == 'ADASYN':\n",
    "        resampler = ADASYN(random_state=42)\n",
    "        X_train_res, y_train_res = resampler.fit_resample(X_train, y_train)\n",
    "    elif resampling_method == 'Borderline-SMOTE':\n",
    "        resampler = BorderlineSMOTE(random_state=42)\n",
    "        X_train_res, y_train_res = resampler.fit_resample(X_train, y_train)\n",
    "    elif resampling_method == 'SMOTEENN':\n",
    "        resampler = SMOTEENN(random_state=42)\n",
    "        X_train_res, y_train_res = resampler.fit_resample(X_train, y_train)\n",
    "    elif resampling_method == 'SMOTETomek':\n",
    "        resampler = SMOTETomek(random_state=42)\n",
    "        X_train_res, y_train_res = resampler.fit_resample(X_train, y_train)\n",
    "    elif resampling_method == 'oversample':\n",
    "        # Oversample the minority class\n",
    "        X_train_res, y_train_res = resample_minority_class(X_train, y_train)\n",
    "    elif resampling_method == 'undersample':\n",
    "        # Undersample the majority class\n",
    "        X_train_res, y_train_res = resample_majority_class(X_train, y_train)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid resampling method: {resampling_method}\")\n",
    "\n",
    "    # Step 5: Scale numeric features (optional, based on model requirements)\n",
    "    if scaling_option:\n",
    "        # List of binary columns (do not scale these)\n",
    "        binary_cols = ['Gender', 'Married', 'Education', 'Self_Employed', 'Credit_History']\n",
    "        # Include 'Property_Area' dummies\n",
    "        binary_cols += [col for col in X_train_res.columns if 'Property_Area_' in col]\n",
    "\n",
    "        # Columns to scale\n",
    "        cols_to_scale = [col for col in X_train_res.columns if col not in binary_cols]\n",
    "\n",
    "        # Apply scaler\n",
    "        scaler = None\n",
    "        if scaling_option == 'standard':\n",
    "            scaler = StandardScaler()\n",
    "        elif scaling_option == 'minmax':\n",
    "            scaler = MinMaxScaler()\n",
    "        elif scaling_option == 'robust':\n",
    "            scaler = RobustScaler()\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid scaling option: {scaling_option}\")\n",
    "\n",
    "        if scaler:\n",
    "            # Ensure that X_train_res, X_test, test are DataFrames\n",
    "            if not isinstance(X_train_res, pd.DataFrame):\n",
    "                X_train_res = pd.DataFrame(X_train_res, columns=X.columns)\n",
    "            if not isinstance(X_test, pd.DataFrame):\n",
    "                X_test = pd.DataFrame(X_test, columns=X.columns)\n",
    "            if not isinstance(test, pd.DataFrame):\n",
    "                test = pd.DataFrame(test, columns=X.columns)\n",
    "\n",
    "            # Apply scaling only to continuous features\n",
    "            X_train_res[cols_to_scale] = scaler.fit_transform(X_train_res[cols_to_scale])\n",
    "            X_test[cols_to_scale] = scaler.transform(X_test[cols_to_scale])\n",
    "            test[cols_to_scale] = scaler.transform(test[cols_to_scale])\n",
    "\n",
    "    # Convert back to arrays for modeling\n",
    "    X_train_res = X_train_res.values\n",
    "    X_test = X_test.values\n",
    "    test_scaled = test.values\n",
    "\n",
    "    return X_train_res, y_train_res, X_test, y_test, test_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def preprocess_data(train, test):\n",
    "    # Step 1: Drop the 'Loan_ID' column\n",
    "    train = train.drop('Loan_ID', axis=1)\n",
    "    test = test.drop('Loan_ID', axis=1)\n",
    "\n",
    "    # Step 2: Replace '3+' with 3 in the 'Dependents' column\n",
    "    train['Dependents'] = train['Dependents'].replace('3+', '3')\n",
    "    test['Dependents'] = test['Dependents'].replace('3+', '3')\n",
    "\n",
    "    # Step 3: Convert 'Dependents' to numeric datatype\n",
    "    train['Dependents'] = pd.to_numeric(train['Dependents'])\n",
    "    test['Dependents'] = pd.to_numeric(test['Dependents'])\n",
    "\n",
    "    # Handle missing values before transformations\n",
    "    numeric_cols = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term', 'Total_Income']\n",
    "    for col in numeric_cols:\n",
    "        train[col].fillna(train[col].median(), inplace=True)\n",
    "        test[col].fillna(test[col].median(), inplace=True)\n",
    "\n",
    "    # Step 4: Apply log transformation to 'ApplicantIncome' to create additional features\n",
    "    train['Log_ApplicantIncome'] = np.log1p(train['ApplicantIncome'])\n",
    "    test['Log_ApplicantIncome'] = np.log1p(test['ApplicantIncome'])\n",
    "\n",
    "    # Step 5: Apply log transformation to 'CoapplicantIncome' to create additional features\n",
    "    train['Log_CoapplicantIncome'] = np.log1p(train['CoapplicantIncome'])\n",
    "    test['Log_CoapplicantIncome'] = np.log1p(test['CoapplicantIncome'])\n",
    "\n",
    "    # Step 6: Apply log transformation to 'LoanAmount' to create additional features\n",
    "    train['Log_LoanAmount'] = np.log1p(train['LoanAmount'])\n",
    "    test['Log_LoanAmount'] = np.log1p(test['LoanAmount'])\n",
    "\n",
    "    # Step 7: Apply Box-Cox transformation to 'Loan_Amount_Term' to create additional features\n",
    "    # Box-Cox requires all positive values; ensure no zeros or negatives\n",
    "    train['Loan_Amount_Term'] = train['Loan_Amount_Term'].replace(0, train['Loan_Amount_Term'].median())\n",
    "    test['Loan_Amount_Term'] = test['Loan_Amount_Term'].replace(0, test['Loan_Amount_Term'].median())\n",
    "\n",
    "    # Apply Box-Cox transformation\n",
    "    train['BoxCox_Loan_Amount_Term'], lam = boxcox(train['Loan_Amount_Term'])\n",
    "    test['BoxCox_Loan_Amount_Term'] = boxcox(test['Loan_Amount_Term'], lmbda=lam)\n",
    "\n",
    "    # Step 8: Apply log transformation to 'Total_Income' to create additional features\n",
    "    train['Log_Total_Income'] = np.log1p(train['Total_Income'])\n",
    "    test['Log_Total_Income'] = np.log1p(test['Total_Income'])\n",
    "\n",
    "    # Steps 11 & 12: Drop 'Gender' and 'Married' columns if they have no effect\n",
    "    train = train.drop(['Gender', 'Married'], axis=1)\n",
    "    test = test.drop(['Gender', 'Married'], axis=1)\n",
    "\n",
    "    # Step 9: Create X and y, then split the train data into train, validation, and test sets\n",
    "    X = train.drop('Loan_Status', axis=1)\n",
    "    y = train['Loan_Status']\n",
    "\n",
    "    # Encode categorical variables (if any remain after dropping)\n",
    "    X = pd.get_dummies(X, drop_first=True)\n",
    "    test = pd.get_dummies(test, drop_first=True)\n",
    "\n",
    "    # Align test set columns with training set\n",
    "    X, test = X.align(test, join='left', axis=1, fill_value=0)\n",
    "\n",
    "    # Step 10: Perform resampling of the minority class using SMOTE\n",
    "    sm = SMOTE(random_state=25)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Step 13: Scale the features using StandardScaler for PyTorch\n",
    "    scaler = StandardScaler()\n",
    "    X_train_res = scaler.fit_transform(X_train_res)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    test = scaler.transform(test)\n",
    "\n",
    "    return X_train_res, y_train_res, X_test, y_test, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/Train.csv')\n",
    "test = pd.read_csv('data/Test.csv')\n",
    "\n",
    "# Preprocess the data\n",
    "train_processed, test_processed = preprocess_data(train, test)\n",
    "\n",
    "# Resample, split, and scale the data\n",
    "X_train_res, y_train_res, X_test, y_test, test_scaled = resample_split(\n",
    "    train_processed,\n",
    "    test_processed,\n",
    "    resampling_method='SMOTE',\n",
    "    scaling_option='standard'\n",
    ")\n",
    "\n",
    "# X_train_res, y_train_res, X_test, y_test, test_scaled = preprocess_data(train, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_lightgbm(X_train, y_train):\n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'num_leaves': [31, 50, 70],\n",
    "        'min_data_in_leaf': [20, 50, 100]\n",
    "    }\n",
    "\n",
    "    # Create LightGBM model\n",
    "    lgbm = lgb.LGBMClassifier(boosting_type='gbdt', objective='binary', random_state=42)\n",
    "\n",
    "    # Stratified K-Fold\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Grid Search\n",
    "    grid_search = GridSearchCV(estimator=lgbm, param_grid=param_grid, scoring='roc_auc', cv=skf, n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best LightGBM Parameters:\", grid_search.best_params_)\n",
    "    print(\"Best LightGBM ROC-AUC:\", grid_search.best_score_)\n",
    "\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "def grid_search_xgboost(X_train, y_train):\n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'subsample': [0.7, 0.8, 0.9],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "    }\n",
    "\n",
    "    # Create XGBoost model\n",
    "    xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "    # Stratified K-Fold\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Grid Search\n",
    "    grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='roc_auc', cv=skf, n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best XGBoost Parameters:\", grid_search.best_params_)\n",
    "    print(\"Best XGBoost ROC-AUC:\", grid_search.best_score_)\n",
    "\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "def grid_search_catboost(X_train, y_train):\n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'iterations': [500, 1000],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'depth': [4, 6, 8],\n",
    "        'l2_leaf_reg': [1, 3, 5]\n",
    "    }\n",
    "\n",
    "    # Create CatBoost model\n",
    "    catboost_model = CatBoostClassifier(verbose=0, random_state=42, eval_metric='AUC', task_type='CPU')\n",
    "\n",
    "    # Stratified K-Fold\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Grid Search\n",
    "    grid_search = GridSearchCV(estimator=catboost_model, param_grid=param_grid, scoring='roc_auc', cv=skf, n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best CatBoost Parameters:\", grid_search.best_params_)\n",
    "    print(\"Best CatBoost ROC-AUC:\", grid_search.best_score_)\n",
    "\n",
    "    return grid_search.best_estimator_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LightGBM Grid Search\n",
    "best_lgb_model = grid_search_lightgbm(X_train_res, y_train_res)\n",
    "\n",
    "# # XGBoost Grid Search\n",
    "# best_xgb_model = grid_search_xgboost(X_train_res, y_train_res)\n",
    "\n",
    "# # CatBoost Grid Search\n",
    "# best_cat_model = grid_search_catboost(X_train_res, y_train_res)\n",
    "\n",
    "# Evaluate on test data (Example using LightGBM)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_pred = best_lgb_model.predict_proba(X_test)[:, 1]\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "print(\"LightGBM Test ROC-AUC:\", roc_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict on the test set\n",
    "y_test_pred = best_lgb_model.predict(X_test)\n",
    "\n",
    "# Function to print metrics\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"Metrics for {dataset_name}:\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(\"Precision:\", precision_score(y_true, y_pred, pos_label=1))  # Change 'Yes' to the positive label in your dataset\n",
    "    print(\"Recall:\", recall_score(y_true, y_pred, pos_label=1))\n",
    "    print(\"F1 Score:\", f1_score(y_true, y_pred, pos_label=1))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Print metrics for test set\n",
    "print_metrics(y_test, y_test_pred, \"Test Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def objective_lightgbm(trial):\n",
    "    # Suggest hyperparameters\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.1),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 10, 100),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.5, 0.9),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.5, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "    }\n",
    "\n",
    "    # Stratified K-Fold Cross-Validation\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = []\n",
    "\n",
    "    for train_idx, val_idx in skf.split(X_train_res, y_train_res):\n",
    "        X_train_fold, X_val_fold = X_train_res[train_idx], X_train_res[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train_res[train_idx], y_train_res[val_idx]\n",
    "\n",
    "        model = LGBMClassifier(**params, random_state=42)\n",
    "        model.fit(X_train_fold, y_train_fold, eval_set=[(X_val_fold, y_val_fold)], early_stopping_rounds=50, verbose=False)\n",
    "        preds = model.predict_proba(X_val_fold)[:, 1]\n",
    "        cv_scores.append(roc_auc_score(y_val_fold, preds))\n",
    "\n",
    "    # Return the average ROC-AUC across folds\n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "# Create Optuna study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# Use tqdm for progress tracking\n",
    "n_trials = 50\n",
    "start_time = time.time()\n",
    "with tqdm(total=n_trials, desc=\"Optimizing LightGBM\", unit=\"trial\") as pbar:\n",
    "    def callback(study, trial):\n",
    "        pbar.update(1)\n",
    "    \n",
    "    study.optimize(objective_lightgbm, n_trials=n_trials, callbacks=[callback])\n",
    "\n",
    "# Calculate total time\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Total Optimization Time (LightGBM): {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters (LightGBM):\", study.best_params)\n",
    "print(\"Best AUC (LightGBM):\", study.best_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def objective_lightgbm(trial):\n",
    "    # Suggest hyperparameters\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.1),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.5, 0.9),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.5, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "    }\n",
    "\n",
    "    # Stratified K-Fold Cross-Validation\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = []\n",
    "\n",
    "    for train_idx, val_idx in skf.split(X_train_res, y_train_res):\n",
    "        X_train_fold, X_val_fold = X_train_res[train_idx], X_train_res[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train_res[train_idx], y_train_res[val_idx]\n",
    "\n",
    "        # Instantiate the model with suggested parameters\n",
    "        model = LGBMClassifier(**params)\n",
    "\n",
    "        # Fit the model with early stopping\n",
    "        model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            eval_set=[(X_val_fold, y_val_fold)],\n",
    "            eval_names=['valid'],\n",
    "            eval_metric='auc',\n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False  # Enable verbose output\n",
    "        )\n",
    "\n",
    "        # Predict probabilities for validation set\n",
    "        preds = model.predict_proba(X_val_fold)[:, 1]\n",
    "        cv_scores.append(roc_auc_score(y_val_fold, preds))\n",
    "\n",
    "    # Return the average ROC-AUC across folds\n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "# Create Optuna study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# Use tqdm for progress tracking\n",
    "n_trials = 50\n",
    "start_time = time.time()\n",
    "with tqdm(total=n_trials, desc=\"Optimizing LightGBM\", unit=\"trial\") as pbar:\n",
    "    def callback(study, trial):\n",
    "        pbar.update(1)\n",
    "    \n",
    "    study.optimize(objective_lightgbm, n_trials=n_trials, callbacks=[callback])\n",
    "\n",
    "# Calculate total time\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Total Optimization Time (LightGBM): {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters (LightGBM):\", study.best_params)\n",
    "print(\"Best AUC (LightGBM):\", study.best_value)\n",
    "\n",
    "# Train final model with best parameters\n",
    "best_params = study.best_params\n",
    "final_model_lgb = LGBMClassifier(**best_params, random_state=42)\n",
    "final_model_lgb.fit(X_train_res, y_train_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_test_pred = final_model_lgb.predict(X_test)\n",
    "\n",
    "# Function to print metrics\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    print(f\"Metrics for {dataset_name}:\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(\"Precision:\", precision_score(y_true, y_pred, pos_label=1))  # Change 'Yes' to the positive label in your dataset\n",
    "    print(\"Recall:\", recall_score(y_true, y_pred, pos_label=1))\n",
    "    print(\"F1 Score:\", f1_score(y_true, y_pred, pos_label=1))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Print metrics for test set\n",
    "print_metrics(y_test, y_test_pred, \"Test Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "print(\"LightGBM Version:\", lgb.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
